import{_ as o,c as t,o as s,ag as a}from"./chunks/framework.Ds6Eueu6.js";const h=JSON.parse('{"title":"Using Resource Groups","description":"","frontmatter":{},"headers":[],"relativePath":"docs/7x/admin_guide/workload_mgmt_resgroups.md","filePath":"docs/7x/admin_guide/workload_mgmt_resgroups.md"}'),r={name:"docs/7x/admin_guide/workload_mgmt_resgroups.md"};function i(n,e,u,c,p,d){return s(),t("div",null,e[0]||(e[0]=[a(`<h1 id="using-resource-groups" tabindex="-1">Using Resource Groups <a class="header-anchor" href="#using-resource-groups" aria-label="Permalink to &quot;Using Resource Groups&quot;">​</a></h1><hr><p>You use resource groups to manage and protect the resource allocation of CPU, memory, concurrent transaction limits, and disk I/O in WarehousePG. Once you define a resource group, you assign the group to one or more WarehousePG roles, or to an external component such as PL/Container, in order to control the resources used by them.</p><p>When you assign a resource group to a role, the resource limits that you define for the group apply to all of the roles to which you assign the group. For example, the memory limit for a resource group identifies the maximum memory usage for all running transactions submitted by WarehousePG users in all roles to which you assign the group.</p><p>WarehousePG uses Linux-based control groups for CPU resource management, and Runaway Detector for statistics, tracking and management of memory.</p><p>When using resource groups to control resources like CPU cores, review the Hyperthreading note in <a href="./../install_guide/platform-requirements.html#hardware-requirements">Hardware and Network</a>.</p><p><strong>Parent topic:</strong> <a href="./wlmgmt.html">Managing Resources</a></p><h2 id="understanding-role-and-component-resource-groups" tabindex="-1"><a id="topic8339intro"></a>Understanding Role and Component Resource Groups <a class="header-anchor" href="#understanding-role-and-component-resource-groups" aria-label="Permalink to &quot;&lt;a id=&quot;topic8339intro&quot;&gt;&lt;/a&gt;Understanding Role and Component Resource Groups&quot;">​</a></h2><p>WarehousePG supports two types of resource groups: groups that manage resources for roles, and groups that manage resources for external components such as PL/Container.</p><p>The most common application for resource groups is to manage the number of active queries that different roles may run concurrently in your WarehousePG cluster. You can also manage the amount of CPU, memory resources, and disk I/O that WarehousePG allocates to each query.</p><p>When a user runs a query, WarehousePG evaluates the query against a set of limits defined for the resource group. WarehousePG runs the query immediately if the group&#39;s resource limits have not yet been reached and the query does not cause the group to exceed the concurrent transaction limit. If these conditions are not met, WarehousePG queues the query. For example, if the maximum number of concurrent transactions for the resource group has already been reached, a subsequent query is queued and must wait until other queries complete before it runs. WarehousePG may also run a pending query when the resource group&#39;s concurrency and memory limits are altered to large enough values.</p><p>Within a resource group for roles, transactions are evaluated on a first in, first out basis. WarehousePG periodically assesses the active workload of the system, reallocating resources and starting/queuing jobs as necessary.</p><p>You can also use resource groups to manage the CPU and memory resources of external components such as PL/Container. Resource groups for external components use Linux cgroups to manage the total CPU resources for the component.</p><h2 id="resource-group-attributes-and-limits" tabindex="-1"><a id="topic8339introattrlim"></a>Resource Group Attributes and Limits <a class="header-anchor" href="#resource-group-attributes-and-limits" aria-label="Permalink to &quot;&lt;a id=&quot;topic8339introattrlim&quot;&gt;&lt;/a&gt;Resource Group Attributes and Limits&quot;">​</a></h2><p>When you create a resource group, you provide a set of limits that determine the amount of CPU and memory resources available to the group. The following table lists the available limits for resource groups:</p><table tabindex="0"><thead><tr><th>Limit Type</th><th>Description</th><th>Value Range</th><th>Default</th></tr></thead><tbody><tr><td>CONCURRENCY</td><td>The maximum number of concurrent transactions, including active and idle transactions, that are permitted in the resource group.</td><td>[0 - <a href="./../ref_guide/config_params/guc-list.html#max_connections">max_connections</a>]</td><td>20</td></tr><tr><td>CPU_MAX_PERCENT</td><td>The maximum percentage of CPU resources the group can use.</td><td>[1 - 100]</td><td>-1 (not set)</td></tr><tr><td>CPU_WEIGHT</td><td>The scheduling priority of the resource group.</td><td>[1 - 500]</td><td>100</td></tr><tr><td>CPUSET</td><td>The specific CPU logical core (or logical thread in hyperthreading) reserved for this resource group.</td><td>It depends on system core configuration</td><td>-1</td></tr><tr><td>IO_LIMIT</td><td>The limit for the maximum read/write disk I/O throughput, and maximum read/write I/O operations per second. Set the value on a per-tablespace basis.</td><td>[2 - 4294967295 or <code>max</code>]</td><td>-1</td></tr><tr><td>MEMORY_QUOTA</td><td>The memory limit value specified for the resource group.</td><td>Integer (MB)</td><td>-1 (not set, use <code>statement_mem</code> as the memory limit for a single query)</td></tr><tr><td>MIN_COST</td><td>The minimum cost of a query plan to be included in the resource group.</td><td>Integer</td><td>0</td></tr></tbody></table><blockquote><p><strong>Note</strong> Resource limits are not enforced on <code>SET</code>, <code>RESET</code>, and <code>SHOW</code> commands.</p></blockquote><h3 id="transaction-concurrency-limit" tabindex="-1"><a id="topic8339717179"></a>Transaction Concurrency Limit <a class="header-anchor" href="#transaction-concurrency-limit" aria-label="Permalink to &quot;&lt;a id=&quot;topic8339717179&quot;&gt;&lt;/a&gt;Transaction Concurrency Limit&quot;">​</a></h3><p>The <code>CONCURRENCY</code> limit controls the maximum number of concurrent transactions permitted for a resource group.</p><p>Each resource group is logically divided into a fixed number of slots equal to the <code>CONCURRENCY</code> limit. WarehousePG allocates these slots an equal, fixed percentage of memory resources.</p><p>The default <code>CONCURRENCY</code> limit value for a resource group for roles is 20. A value of 0 means that no query is allowed to run for this resource group.</p><p>WarehousePG queues any transactions submitted after the resource group reaches its <code>CONCURRENCY</code> limit. When a running transaction completes, WarehousePG un-queues and runs the earliest queued transaction if sufficient memory resources exist. Note that if a transaction is in <code>idle in transaction</code> state, even if no statement is running, the concurrency slot is still in use.</p><p>You can set the server configuration parameter <a href="./../ref_guide/config_params/guc-list.html#gp_resource_group_queuing_timeout">gp_resource_group_queuing_timeout</a> to specify the amount of time a transaction remains in the queue before WarehousePG cancels the transaction. The default timeout is zero, WarehousePG queues transactions indefinitely.</p><h3 id="bypass-and-unassign-from-resource-groups" tabindex="-1"><a id="bypass"></a>Bypass and Unassign from Resource Groups <a class="header-anchor" href="#bypass-and-unassign-from-resource-groups" aria-label="Permalink to &quot;&lt;a id=&quot;bypass&quot;&gt;&lt;/a&gt;Bypass and Unassign from Resource Groups&quot;">​</a></h3><p>A query bypasses the resource group concurrency limit if you set the server configuration parameter <a href="./../ref_guide/config_params/guc-list.html#gp_resource_group_bypass">gp_resource_group_bypass</a>. This parameter enables or disables the concurrent transaction limit for the resource group so a query can run immediately. The default value is false, which enforces the limit of the <code>CONCURRENCY</code> limit. You may only set this parameter for a single session, not within a transaction or a function. If you set <code>gp_resource_group_bypass</code> to true, the query no longer enforces the CPU or memory limits assigned to its resource group. Instead, the memory quota assigned to this query is <code>statement_mem</code> per query. If there is not enough memory to satisfy the memory allocation request, the query will fail.</p><p>You may bypass queries that only use catalog tables, such as the database Graphical User Interface (GUI) client, which runs catalog queries to obtain metadata. If the server configuration parameter <a href="./../ref_guide/config_params/guc-list.html#gp_resource_group_bypass_catalog_query">gp_resource_group_bypass_catalog_query</a> is set to true (the default), WarehousePG&#39;s resource group scheduler bypasses all queries that read exclusively from system catalogs, or queries that contain in their query text <code>pg_catalog</code> schema tables only. These queries are automatically unassigned from its current resource group; they do not enforce the limits of the resource group and do not account for resource usage. The query resources are assigned out of the resource groups and run immediately. The memory quota is <code>statement_mem</code> per the query.</p><p>You may bypass direct dispatch queries with the server configuration parameter <a href="./../ref_guide/config_params/guc-list.html#gp_resource_group_bypass_direct_dispatch">gp_resource_group_bypass_direct_dispatch</a>. A direct dispatch query is a special type of query that only requires a single segment to participate in the execution. In order to improve efficiency, WarehousePG optimizes this type of query, using direct dispatch optimization. The system sends the query plan to the execution of a single segment that needs to execute the plan, instead of sending it to all segments for execution. If you set <code>gp_resource_group_bypass_direct_dispatch</code> to true, the query no longer enforces the CPU or memory limits assigned to its resource group, so it runs immediately. Instead, the memory quota assigned to this query is <code>statement_mem</code> per query. If there is not enough memory to satisfy the memory allocation request, the query will fail. You may only set this parameter for a single session, not within a transaction or a function.</p><p>Queries whose plan cost is less than the limit <code>MIN_COST</code> are automatically unassigned from their resource group and do not enforce any of its limits. The resources used by the query do not account for the resources of the resource group. The query has a memory quota of <code>statement_mem</code>. The default value of <code>MIN_COST</code> is 0.</p><h3 id="cpu-limits" tabindex="-1"><a id="topic833971717"></a>CPU Limits <a class="header-anchor" href="#cpu-limits" aria-label="Permalink to &quot;&lt;a id=&quot;topic833971717&quot;&gt;&lt;/a&gt;CPU Limits&quot;">​</a></h3><p>WarehousePG leverages Linux control groups to implement CPU resource management. WarehousePG allocates CPU resources in two ways:</p><ul><li>By assigning a percentage of CPU resources to resource groups.</li><li>By assigning specific CPU cores to resource groups</li></ul><p>When you set one of the allocation modes for a resource group, WarehousePG deactivates the other allocation mode. You may employ both modes of CPU resource allocation simultaneously for different resource groups on the same WarehousePG cluster. You may also change the CPU resource allocation mode for a resource group at runtime.</p><p>WarehousePG uses the server configuration parameter <a href="./../ref_guide/config_params/guc-list.html#gp_resource_group_cpu_limit">gp_resource_group_cpu_limit</a> to identify the maximum percentage of system CPU resources to allocate to resource groups on each WarehousePG segment node. The remaining unreserved CPU resources are used for the operating system kernel and WarehousePG daemons. The amount of CPU available to WarehousePG queries per host is then divided equally among each segment on the WarehousePG node.</p><blockquote><p><strong>Note</strong> The default <code>gp_resource_group_cpu_limit</code> value may not leave sufficient CPU resources if you are running other workloads on your WarehousePG cluster nodes, so be sure to adjust this server configuration parameter accordingly.</p></blockquote><blockquote><p><strong>Caution</strong> Avoid setting <code>gp_resource_group_cpu_limit</code> to a value higher than .9. Doing so may result in high workload queries taking near all CPU resources, potentially starving WarehousePG auxiliary processes.</p></blockquote><h4 id="assigning-cpu-resources-by-core" tabindex="-1"><a id="cpuset"></a>Assigning CPU Resources by Core <a class="header-anchor" href="#assigning-cpu-resources-by-core" aria-label="Permalink to &quot;&lt;a id=&quot;cpuset&quot;&gt;&lt;/a&gt;Assigning CPU Resources by Core&quot;">​</a></h4><p>You identify the CPU cores that you want to reserve for a resource group with the <code>CPUSET</code> property. The CPU cores that you specify must be available in the system and cannot overlap with any CPU cores that you reserved for other resource groups. Although WarehousePG uses the cores that you assign to a resource group exclusively for that group, note that those CPU cores may also be used by non-WarehousePG processes in the system. When you configure <code>CPUSET</code> for a resource group, WarehousePG deactivates <code>CPU_MAX_PERCENT</code> and <code>CPU_WEIGHT</code> for the group and sets their value to -1.</p><p>Specify CPU cores separately for the coordinator host and segment hosts, separated by a semicolon. Use a comma-separated list of single core numbers or number intervals when you configure cores for <code>CPUSET</code>. You must enclose the core numbers/intervals in single quotes, for example, &#39;1;1,3-4&#39; uses core 1 on the coordinator host, and cores 1, 3, and 4 on segment hosts.</p><p>When you assign CPU cores to <code>CPUSET</code> groups, consider the following:</p><ul><li>A resource group that you create with <code>CPUSET</code> uses the specified cores exclusively. If there are no running queries in the group, the reserved cores are idle and cannot be used by queries in other resource groups. Consider minimizing the number of <code>CPUSET</code> groups to avoid wasting system CPU resources.</li><li>Consider keeping CPU core 0 unassigned. CPU core 0 is used as a fallback mechanism in the following cases: <ul><li><code>admin_group</code> and <code>default_group</code> require at least one CPU core. When all CPU cores are reserved, WarehousePG assigns CPU core 0 to these default groups. In this situation, the resource group to which you assigned CPU core 0 shares the core with <code>admin_group</code> and <code>default_group</code>.</li><li>If you restart your WarehousePG cluster with one node replacement and the node does not have enough cores to service all <code>CPUSET</code> resource groups, the groups are automatically assigned CPU core 0 to avoid system start failure.</li></ul></li><li>Use the lowest possible core numbers when you assign cores to resource groups. If you replace a WarehousePG node and the new node has fewer CPU cores than the original, or if you back up the database and want to restore it on a cluster with nodes with fewer CPU cores, the operation may fail. For example, if your WarehousePG cluster has 16 cores, assigning cores 1-7 is optimal. If you create a resource group and assign CPU core 9 to this group, database restore to an 8 core node will fail.</li></ul><p>Resource groups that you configure with <code>CPUSET</code> have a higher priority on CPU resources. The maximum CPU resource usage percentage for all resource groups configured with <code>CPUSET</code> on a segment host is the number of CPU cores reserved divided by the number of all CPU cores, multiplied by 100.</p><blockquote><p><strong>Note</strong> You must configure <code>CPUSET</code> for a resource group <em>after</em> you have enabled resource group-based resource management for your WarehousePG cluster with the <a href="./../ref_guide/config_params/guc-list.html#gp_resource_manager">gp_resource_manager</a> server configuration parameter.</p></blockquote><h4 id="assigning-cpu-resources-by-percentage" tabindex="-1"><a id="cpu_max_percent"></a>Assigning CPU Resources by Percentage <a class="header-anchor" href="#assigning-cpu-resources-by-percentage" aria-label="Permalink to &quot;&lt;a id=&quot;cpu_max_percent&quot;&gt;&lt;/a&gt;Assigning CPU Resources by Percentage&quot;">​</a></h4><p>You configure a resource group with <code>CPU_MAX_PERCENT</code> in order to assign CPU resources by percentage. When you configure <code>CPU_MAX_PERCENT</code> for a resource group, WarehousePG deactivates <code>CPUSET</code> for the group.</p><p>The parameter <code>CPU_MAX_PERCENT</code> sets a hard upper limit for the percentage of the segment CPU for resource management. The minimum <code>CPU_MAX_PERCENT</code> percentage you can specify for a resource group is 1, the maximum is 100. The sum of <code>CPU_MAX_PERCENT</code>s specified for all resource groups that you define in your WarehousePG cluster can exceed 100. It specifies the total time ratio that all tasks in a resource group can run in a given CPU time period. Once the tasks in the resource group have used up all the time specified by the quota, they are throttled for the remainder of the time specified in that time period, and are not allowed to run until the next time period.</p><p>When tasks in a resource group are idle and not using any CPU time, the leftover time is collected in a global pool of unused CPU cycles. Other resource groups can borrow CPU cycles from this pool. The actual amount of CPU time available to a resource group may vary, depending on the number of resource groups present on the system.</p><p>The parameter <code>CPU_MAX_PERCENT</code> enforces a hard upper limit of CPU usage. For example, if it is set to 40%, it indicates that although the resource group can temporarily use some idle CPU resources from other groups, the maximum it can use is 40% of the CPU resources available to WarehousePG.</p><p>You set the parameter <code>CPU_WEIGHT</code> to assign the scheduling priority of the current group. The default value is 100, and the range of values is 1 to 500. The value specifies the relative share of CPU time available to tasks in the resource group. For example, if one resource group has a relative share of 100 and another two groups have a relative share of 50, when processes in all the resource groups try to use 100% of the CPU (that means, the value of <code>CPU_MAX_PERCENT</code> for all groups is set to 100), the first resource group gets 50% of all CPU time, and the other two get 25% each. However, if you add another group with a relative share of 100, the first group is only allowed to use 33% of the CPU, and the remaining groups get 16.5%, 16.5%, and 33% respectively.</p><p>For example, consider the following groups:</p><table tabindex="0"><thead><tr><th>Group Name</th><th>CONCURRENCY</th><th>CPU_MAX_PERCENT</th><th>CPU_WEIGHT</th></tr></thead><tbody><tr><td>default_group</td><td>20</td><td>50</td><td>10</td></tr><tr><td>admin_group</td><td>10</td><td>70</td><td>30</td></tr><tr><td>system_group</td><td>10</td><td>30</td><td>10</td></tr><tr><td>test</td><td>10</td><td>10</td><td>10</td></tr></tbody></table><p>Roles in <code>default_group</code> have an available CPU ratio (determined by <code>CPU_WEIGHT</code>) of 10/(10+30+10+10)=16%. This means that they can use at least 16% of the CPU when the system workload is high. When the system has idle CPU resources, they can use more resources, as the hard limit (set by <code>CPU_MAX_PERCENT</code>) is 50%.</p><p>Roles in <code>admin_group</code> have an available CPU ratio of 30/(10+30+10+10)=50% when the system workload is high. When the system has idle CPU resources, they can use resources up to the hard limit of 70%.</p><p>Roles in <code>test</code> have a CPU ratio of 10/(10+30+10+10)=16%. However, as the hard limit determined by <code>CPU_MAX_PERCENT</code> is 10%, they can only use up to 10% of the resources even when the system is idle.</p><h3 id="memory-limits" tabindex="-1"><a id="topic8339717"></a>Memory Limits <a class="header-anchor" href="#memory-limits" aria-label="Permalink to &quot;&lt;a id=&quot;topic8339717&quot;&gt;&lt;/a&gt;Memory Limits&quot;">​</a></h3><p>When you enable resource groups, memory usage is managed at the WarehousePG segment and resource group levels. You can also manage memory at the transaction level. See <a href="./wlmgmt_intro.html">WarehousePG Memory Overview</a> to estimate how much memory each WarehousePG segment has available to use. This will help you estimate how much memory to assign to the resource groups.</p><p>The amount of memory allocated to a query is determined by the following parameters:</p><p>The parameter <code>MEMORY_QUOTA</code> of a resource group sets the maximum amount of memory reserved for this resource group on a segment. This determines the total amount of memory that all worker processes for a query can consume on the segment host during query execution. The amount of memory allotted to a query is the group memory limit divided by the group concurrency limit: <code>MEMORY_QUOTA</code> / <code>CONCURRENCY</code>.</p><p>If a query requires a large amount of memory, you may use the server configuration parameter <a href="./../ref_guide/config_params/guc-list.html#gp_resgroup_memory_query_fixed_mem">gp_resgroup_memory_query_fixed_mem</a> to set a fixed memory amount for the query at the session level. This parameter overrides and can surpass the allocated memory of the resource group.</p><p>WarehousePG allocates memory for an incoming query using the <code>gp_resgroup_memory_query_fixed_mem</code> value, if set, to bypass the resource group settings. Otherwise, it uses <code>MEMORY_QUOTA</code> / <code>CONCURRENCY</code> as the memory allocated for the query. If <code>MEMORY_QUOTA</code> is not set, the value for the query memory allocation defaults to <a href="./../ref_guide/config_params/guc-list.html#statement_mem">statement_mem</a>.</p><p>For all queries, if there is not enough memory in the system, they spill to disk. When the limit <a href="./../ref_guide/config_params/guc-list.html#gp_workfile_limit_files_per_query">gp_workfile_limit_files_per_query</a> is reached, WarehousePG generates an out of memory (OOM) error.</p><p>For example, consider a resource group named <code>adhoc</code> with <code>MEMORY_QUOTA</code>set to 1.5 GB and <code>CONCURRENCY</code> set to 3. By default, each statement submitted to the group is allocated 500 MB of memory. Now consider the following series of events:</p><ol><li><p>User <code>ADHOC_1</code> submits query <code>Q1</code>, overriding <code>gp_resgroup_memory_query_fixed_mem</code> to 800MB. The <code>Q1</code> statement is admitted into the system.</p></li><li><p>User <code>ADHOC_2</code> submits query <code>Q2</code>, using the default 500MB.</p></li><li><p>With <code>Q1</code> and <code>Q2</code> still running, user <code>ADHOC3</code> submits query <code>Q3</code>, using the default 500MB.</p><p>Queries <code>Q1</code> and <code>Q2</code> have used 1300MB of the group&#39;s 1500MB. However, if there is enough system memory available for query <code>Q3</code> in the segment at that time, it can run normally.</p></li><li><p>User <code>ADHOC4</code> submits query <code>Q4</code>, using <code>gp_resgroup_memory_query_fixed_mem</code> set to 700 MB.</p><p>Query <code>Q4</code> runs immediately as it bypasses the resource group limits.</p></li></ol><p>There are some special usage considerations regarding memory limits:</p><ul><li>If you set the configuration parameters <code>gp_resource_group_bypass</code> or <code>gp_resource_group_bypass_catalog_query</code> to bypass the resource group limits, the memory limit for the query takes the value of <code>statement_mem</code>.</li><li>When (<code>MEMORY_QUOTA</code> / <code>CONCURRENCY</code>) &lt; <code>statement_mem</code>, WarehousePG uses <code>statement_mem</code> as the fixed amount of memory allocated by query.</li><li>The maximum value of <code>statement_mem</code> is capped at <a href="./../ref_guide/config_params/guc-list.html#max_statement_mem">max_statement_mem</a>.</li><li>Queries whose plan cost is less than the limit <code>MIN_COST</code> use a memory quota of <code>statement_mem</code>.</li></ul><h3 id="disk-i-o-limits" tabindex="-1"><a id="diskio"></a>Disk I/O Limits <a class="header-anchor" href="#disk-i-o-limits" aria-label="Permalink to &quot;&lt;a id=&quot;diskio&quot;&gt;&lt;/a&gt;Disk I/O Limits&quot;">​</a></h3><p>WarehousePG leverages Linux control groups to implement disk I/O limits. The parameter <code>IO_LIMIT</code> limits the maximum read/write disk I/O throughput, and the maximum read/write I/O operations per second for the queries assigned to a specific resource group. It allocates bandwidth, ensures the use of high-priority resource groups, and avoids excessive use of disk bandwidth. The value of the parameter is set on a per-tablespace basis.</p><blockquote><p><strong>Note</strong> Disk I/O limits are only available when you use Linux Control Groups v2. See <a href="#topic71717999">Configuring and Using Resource Groups</a> for more information.</p></blockquote><p>When you limit disk I/O you specify:</p><ul><li><p>The tablespace name or the tablespace object ID (OID) you set the limits for. Use <code>*</code> to set limits for all tablespaces.</p></li><li><p>The values for <code>rbps</code> and <code>wbps</code> to limit the maximum read and write disk I/O throughput in the resource group, in MB/sec. The default value is <code>max</code>, which means there is no limit.</p></li><li><p>The values for <code>riops</code> and <code>wiops</code> to limit the maximum read and write I/O operations per second in the resource group. The default value is <code>max</code>, which means there is no limit.</p></li></ul><p>If the parameter <code>IO_LIMIT</code> is not set, the default value for <code>rbps</code>, <code>wpbs</code>, <code>riops</code>, and <code>wiops</code>s is set to <code>max</code>, which means that there are no disk I/O limits. In this scenario, the <code>gp_toolkit.gp_resgroup_config</code> system view displays its value as <code>-1</code>. If only some of the values of <code>IO_LIMIT</code> are set (for example. <code>rbps</code>), the parameters that are not set default to <code>max</code> (in this example, <code>wbps</code>, <code>riops</code>, wiops\`).</p><h2 id="configuring-and-using-resource-groups" tabindex="-1"><a id="topic71717999"></a>Configuring and Using Resource Groups <a class="header-anchor" href="#configuring-and-using-resource-groups" aria-label="Permalink to &quot;&lt;a id=&quot;topic71717999&quot;&gt;&lt;/a&gt;Configuring and Using Resource Groups&quot;">​</a></h2><h3 id="prerequisites" tabindex="-1"><a id="topic833"></a>Prerequisites <a class="header-anchor" href="#prerequisites" aria-label="Permalink to &quot;&lt;a id=&quot;topic833&quot;&gt;&lt;/a&gt;Prerequisites&quot;">​</a></h3><p>WarehousePG resource groups use Linux Control Groups (cgroups) to manage CPU resources and disk I/O. There are two versions of cgroups: cgroup v1 and cgroup v2. WarehousePG 7 supports both versions, but it only supports the parameter <code>IO_LIMIT</code> for cgroup v2. The version of Linux Control Groups shipped by default with your Linux distribution depends on the operating system version. For Enterprise Linux 8 and older, the default version is v1. For Enterprise Linux 9 and later, the default version is v2. For detailed information about cgroups, refer to the Control Groups documentation for your Linux distribution.</p><p>Verify what version of cgroup is configured in your environment by checking what filesystem is mounted by default during system boot:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>stat -fc %T /sys/fs/cgroup/</span></span></code></pre></div><p>For cgroup v1, the output is <code>tmpfs</code>. For cgroup v2, output is <code>cgroup2fs</code>.</p><p>You do not need to change your version of cgroup, you can simply skip to <a href="#cgroupv1">Configuring cgroup v1</a> or <a href="#cgroupv2">Configuring cgroup v2</a> in order to complete the configuration prerequisites. However, if you prefer to switch from cgroup v1 to v2, run the following commands as root:</p><ul><li>Red Hat 8/Rocky 8/Oracle 8 systems:<div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>grubby --update-kernel=/boot/vmlinuz-$(uname -r) --args=&quot;systemd.unified_cgroup_hierarchy=1&quot;</span></span></code></pre></div></li><li>Ubuntu systems:<div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vim /etc/default/grub</span></span>
<span class="line"><span># add or modify: GRUB_CMDLINE_LINUX=&quot;systemd.unified_cgroup_hierarchy=1&quot;</span></span>
<span class="line"><span>update-grub</span></span></code></pre></div></li></ul><p>If you want to switch from cgroup v2 to v1, run the following commands as root:</p><ul><li>Red Hat 8/Rocky 8/Oracle 8 systems:<div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>grubby --update-kernel=/boot/vmlinuz-$(uname -r) --args=&quot;systemd.unified_cgroup_hierarchy=0 systemd.legacy_systemd_cgroup_controller&quot;</span></span></code></pre></div></li><li>Ubuntu systems:<div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vim /etc/default/grub</span></span>
<span class="line"><span># add or modify: GRUB_CMDLINE_LINUX=&quot;systemd.unified_cgroup_hierarchy=0&quot;</span></span>
<span class="line"><span>update-grub</span></span></code></pre></div></li></ul><p>After that, reboot your host in order for the changes to take effect.</p><h4 id="configuring-cgroup-v1" tabindex="-1"><a id="cgroupv1"></a>Configuring cgroup v1 <a class="header-anchor" href="#configuring-cgroup-v1" aria-label="Permalink to &quot;&lt;a id=&quot;cgroupv1&quot;&gt;&lt;/a&gt;Configuring cgroup v1&quot;">​</a></h4><p>Complete the following tasks on each node in your WarehousePG cluster to set up cgroups v1 for use with resource groups:</p><ol><li><p>Locate the cgroups configuration file <code>/etc/cgconfig.conf</code>. You must be the superuser or have <code>sudo</code> access to edit this file:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vi /etc/cgconfig.conf</span></span></code></pre></div></li><li><p>Add the following configuration information to the file:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>group gpdb {</span></span>
<span class="line"><span>     perm {</span></span>
<span class="line"><span>         task {</span></span>
<span class="line"><span>             uid = gpadmin;</span></span>
<span class="line"><span>             gid = gpadmin;</span></span>
<span class="line"><span>         }</span></span>
<span class="line"><span>         admin {</span></span>
<span class="line"><span>             uid = gpadmin;</span></span>
<span class="line"><span>             gid = gpadmin;</span></span>
<span class="line"><span>         }</span></span>
<span class="line"><span>     }</span></span>
<span class="line"><span>     cpu {</span></span>
<span class="line"><span>     }</span></span>
<span class="line"><span>     cpuacct {</span></span>
<span class="line"><span>     }</span></span>
<span class="line"><span>     cpuset {</span></span>
<span class="line"><span>     }</span></span>
<span class="line"><span>     memory {</span></span>
<span class="line"><span>     }</span></span>
<span class="line"><span>}</span></span></code></pre></div><p>This content configures CPU, CPU accounting, CPU core set, and memory control groups managed by the <code>gpadmin</code> user. WarehousePG uses the memory control group only for monitoring the memory usage.</p></li><li><p>Start the cgroups service on each WarehousePG node. For Redhat/Oracle/Rocky 8.x and older, run the following as root:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>cgconfigparser -l /etc/cgconfig.conf</span></span></code></pre></div></li><li><p>To automatically recreate WarehousePG required cgroup hierarchies and parameters when your system is restarted, configure your system to enable the Linux cgroup service daemon <code>cgconfig.service</code> at node start-up. To ensure the configuration is persisten after reboot, run the following commands as user root. For Redhat/Oracle/Rocky 8.x and older:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>systemctl enable cgconfig.service</span></span></code></pre></div><p>To start the service immediately (without having to reboot) enter:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>systemctl start cgconfig.service</span></span></code></pre></div></li><li><p>Identify the <code>cgroup</code> directory mount point for the node. For Redhat/Oracle/Rocky 8.x and older, run the following as root:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>grep cgroup /proc/mounts</span></span></code></pre></div><p>The first line of output identifies the <code>cgroup</code> mount point.</p></li><li><p>Verify that you set up the WarehousePG cgroups configuration correctly by running the following commands. Replace &lt;cgroup_mount_point&gt; with the mount point that you identified in the previous step:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ls -l &lt;cgroup_mount_point&gt;/cpu/gpdb</span></span>
<span class="line"><span>ls -l &lt;cgroup_mount_point&gt;/cpuset/gpdb</span></span>
<span class="line"><span>ls -l &lt;cgroup_mount_point&gt;/memory/gpdb</span></span></code></pre></div><p>If these directories exist and are owned by <code>gpadmin:gpadmin</code>, you have successfully configured cgroups for WarehousePG resource management.</p></li></ol><h4 id="configuring-cgroup-v2" tabindex="-1"><a id="cgroupv2"></a>Configuring cgroup v2 <a class="header-anchor" href="#configuring-cgroup-v2" aria-label="Permalink to &quot;&lt;a id=&quot;cgroupv2&quot;&gt;&lt;/a&gt;Configuring cgroup v2&quot;">​</a></h4><ol><li><p>Configure the system to mount <code>cgroups-v2</code> by default during system boot by the <code>systemd</code> system and service manager as user root.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>grubby --update-kernel=ALL --args=&quot;systemd.unified_cgroup_hierarchy=1&quot;</span></span></code></pre></div></li><li><p>Reboot the system for the changes to take effect.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>reboot now</span></span></code></pre></div></li><li><p>Create the directory <code>/sys/fs/cgroup/gpdb</code>, add all the necessary controllers, and ensure <code>gpadmin</code> user has read and write permission on it.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>mkdir -p /sys/fs/cgroup/gpdb</span></span>
<span class="line"><span>echo &quot;+cpuset +io +cpu +memory&quot; | tee -a /sys/fs/cgroup/cgroup.subtree_control</span></span>
<span class="line"><span>chown -R gpadmin:gpadmin /sys/fs/cgroup/gpdb</span></span></code></pre></div></li></ol><p>You may encounter the error <code>Invalid argument</code> after running the above commands. This is because cgroups v2 do not support control of real-time processes, and the <code>cpu</code> controller can only be enabled when all the real-time processes are in the root cgroup. In this situation, find all real-time processes and move them to the root cgroup before you re-enable the controllers.</p><ol><li>Ensure that <code>gpadmin</code> has write permission on <code>/sys/fs/cgroup/cgroup.procs</code>. This is required to move the WarehousePG processes from the user slices to <code>/sys/fs/cgroup/gpdb/</code> after the cluster is started in order to manage the postmaster services and all its auxiliary processes.<div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>chmod a+w /sys/fs/cgroup/cgroup.procs</span></span></code></pre></div></li></ol><p>Since resource groups manually manage cgroup files, the above settings will become ineffective after a system reboot. Add the following bash script for systemd so it runs automatically during system startup. Perform the following steps as user root:</p><ol><li><p>Create <code>gpdb.service</code>.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vim /etc/systemd/system/gpdb.service</span></span></code></pre></div></li><li><p>Write the following content into <code>gpdb.service</code>, if the user is not <code>gpadmin</code>, replace it with the appropriate user.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[Unit]</span></span>
<span class="line"><span>Description=WarehousePG Cgroup v2 Configuration Service</span></span>
<span class="line"><span>[Service]</span></span>
<span class="line"><span>Type=simple</span></span>
<span class="line"><span>WorkingDirectory=/sys/fs/cgroup/gpdb.service</span></span>
<span class="line"><span>Delegate=yes</span></span>
<span class="line"><span>Slice=-.slice</span></span>
<span class="line"><span></span></span>
<span class="line"><span># set hierarchies only if cgroup v2 mounted</span></span>
<span class="line"><span>ExecCondition=bash -c &#39;[ xcgroup2fs = x$(stat -fc &quot;%%T&quot; /sys/fs/cgroup) ] || exit 1&#39;</span></span>
<span class="line"><span>ExecStartPre=bash -ec &quot; \\</span></span>
<span class="line"><span>chown -R gpadmin:gpadmin .; \\</span></span>
<span class="line"><span>chmod a+w ../cgroup.procs; \\</span></span>
<span class="line"><span>mkdir -p helper.scope&quot;</span></span>
<span class="line"><span>ExecStart=sleep infinity</span></span>
<span class="line"><span>ExecStartPost=bash -ec &quot;echo $MAINPID &gt; ./helper.scope/cgroup.procs;&quot;</span></span>
<span class="line"><span>[Install]</span></span>
<span class="line"><span>WantedBy=basic.target</span></span></code></pre></div></li><li><p>Reload systemd daemon and enable the service:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>systemctl daemon-reload</span></span>
<span class="line"><span>systemctl enable gpdb.service</span></span></code></pre></div></li></ol><h2 id="enabling-resource-groups" tabindex="-1"><a id="topic8"></a>Enabling Resource Groups <a class="header-anchor" href="#enabling-resource-groups" aria-label="Permalink to &quot;&lt;a id=&quot;topic8&quot;&gt;&lt;/a&gt;Enabling Resource Groups&quot;">​</a></h2><p>When you install WarehousePG, no resource management policy is enabled by default. To use resource groups, set the <a href="./../ref_guide/config_params/guc-list.html#gp_resource_manager">gp_resource_manager</a> server configuration parameter.</p><ol><li><p>Set the <code>gp_resource_manager</code> server configuration parameter to the value <code>&quot;group&quot;</code> or <code>&quot;group-v2&quot;</code>, depending on the version of cgroup configured on your Linux distribution. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>gpconfig -c gp_resource_manager -v &quot;group&quot;</span></span>
<span class="line"><span>gpconfig -c gp_resource_manager -v &quot;group-v2&quot;</span></span></code></pre></div></li><li><p>Restart WarehousePG:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>gpstop</span></span>
<span class="line"><span>gpstart</span></span></code></pre></div></li></ol><p>Once enabled, any transaction submitted by a role is directed to the resource group assigned to the role, and is governed by that resource group&#39;s concurrency, memory, CPU, and disk I/O limits.</p><p>WarehousePG creates three default resource groups for roles named <code>admin_group</code>, <code>default_group</code>, and <code>system_group</code>. When you enable resources groups, any role that was not explicitly assigned a resource group is assigned the default group for the role&#39;s capability. <code>SUPERUSER</code> roles are assigned the <code>admin_group</code>, non-admin roles are assigned the group named <code>default_group</code>. The resources of the WarehousePG cluster processes are assigned to the <code>system_group</code>. You cannot manually assign any roles to the <code>system_group</code>.</p><p>The default resource groups <code>admin_group</code>, <code>default_group</code>, and <code>system_group</code> are created with the following resource limits:</p><table tabindex="0"><thead><tr><th>Limit Type</th><th>admin_group</th><th>default_group</th><th>system_group</th></tr></thead><tbody><tr><td>CONCURRENCY</td><td>10</td><td>5</td><td>0</td></tr><tr><td>CPU_MAX_PERCENT</td><td>10</td><td>20</td><td>10</td></tr><tr><td>CPU_WEIGHT</td><td>100</td><td>100</td><td>100</td></tr><tr><td>CPUSET</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>IO_LIMIT</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>MEMORY_LIMIT</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>MIN_COST</td><td>0</td><td>0</td><td>0</td></tr></tbody></table><h2 id="creating-resource-groups" tabindex="-1"><a id="topic10"></a>Creating Resource Groups <a class="header-anchor" href="#creating-resource-groups" aria-label="Permalink to &quot;&lt;a id=&quot;topic10&quot;&gt;&lt;/a&gt;Creating Resource Groups&quot;">​</a></h2><p>When you create a resource group for a role, you provide a name and a CPU resource allocation mode (core or percentage). You can optionally provide a concurrent transaction limit, a memory limit, a CPU soft priority, disk I/O limits, and a minimum cost. Use the <a href="./../ref_guide/sql_commands/CREATE_RESOURCE_GROUP.html">CREATE RESOURCE GROUP</a> command to create a new resource group.</p><p>When you create a resource group for a role, you must provide a <code>CPU_MAX_PERCENT</code> or <code>CPUSET</code> limit value. These limits identify the percentage of WarehousePG CPU resources to allocate to this resource group. You may specify a <code>MEMORY_QUOTA</code> to reserve a fixed amount of memory for the resource group.</p><p>For example, to create a resource group named <em>rgroup1</em> with a CPU limit of 20, a memory limit of 25, a CPU soft priority of 500, a minimum cost of 50, and disk I/O limits for the <code>pg_default</code> tablespace:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>CREATE RESOURCE GROUP rgroup1 WITH (CONCURRENCY=20, CPU_MAX_PERCENT=20, MEMORY_QUOTA=250, CPU_WEIGHT=500, MIN_COST=50, </span></span>
<span class="line"><span>  IO_LIMIT=’pg_default: wbps=1000, rbps=1000, wiops=100, riops=100’);</span></span></code></pre></div><p>The CPU limit of 20 is shared by every role to which <code>rgroup1</code> is assigned. Similarly, the memory limit of 25 is shared by every role to which <code>rgroup1</code> is assigned. <code>rgroup1</code> utilizes the default <code>CONCURRENCY</code> setting of 20.</p><p>The <a href="./../ref_guide/sql_commands/ALTER_RESOURCE_GROUP.html">ALTER RESOURCE GROUP</a> command updates the limits of a resource group. To change the limits of a resource group, specify the new values that you want for the group. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ALTER RESOURCE GROUP rg_role_light SET CONCURRENCY 7;</span></span>
<span class="line"><span>ALTER RESOURCE GROUP exec SET MEMORY_QUOTA 30;</span></span>
<span class="line"><span>ALTER RESOURCE GROUP rgroup1 SET CPUSET &#39;1;2,4&#39;;</span></span>
<span class="line"><span>ALTER RESOURCE GROUP sales SET IO_LIMIT &#39;tablespace1:wbps=2000,wiops=2000;tablespace2:rbps=2024,riops=2024&#39;;</span></span></code></pre></div><blockquote><p><strong>Note</strong> You cannot set or alter the <code>CONCURRENCY</code> value for the <code>admin_group</code> to zero (0).</p></blockquote><p>The <a href="./../ref_guide/sql_commands/DROP_RESOURCE_GROUP.html">DROP RESOURCE GROUP</a> command drops a resource group. To drop a resource group for a role, the group cannot be assigned to any role, nor can there be any transactions active or waiting in the resource group.</p><p>To drop a resource group:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>DROP RESOURCE GROUP exec;</span></span></code></pre></div><h2 id="configuring-automatic-query-termination-based-on-memory-usage" tabindex="-1"><a id="topic_jlz_hzg_pkb"></a>Configuring Automatic Query Termination Based on Memory Usage <a class="header-anchor" href="#configuring-automatic-query-termination-based-on-memory-usage" aria-label="Permalink to &quot;&lt;a id=&quot;topic_jlz_hzg_pkb&quot;&gt;&lt;/a&gt;Configuring Automatic Query Termination Based on Memory Usage&quot;">​</a></h2><p>WarehousePG supports the Runaway detector, which automatically terminates queries based on the amount of memory used by the query. For resource group-managed queries, WarehousePG terminates a running query based on the amount of memory used by the query. The relevant configuration parameters are:</p><ul><li><p><a href="./../ref_guide/config_params/guc-list.html#gp_vmem_protect_limit">gp_vmem_protect_limit</a> sets the amount of memory that all <code>postgres</code> processes of the active segment instance can consume. If a query causes this limit to be exceeded, no memory will be allocated and the query will fail.</p></li><li><p><a href="./../ref_guide/config_params/guc-list.html#runaway_detector_activation_percent">runaway_detector_activation_percent</a>. When resource groups are enabled, if the used memory exceeds the specified value <code>gp_vmem_protect_limit</code> * <code>runaway_detector_activation_percent</code>, WarehousePG terminates queries based on memory usage, selecting queries from the queries managed by user resource groups (excluding those in the <code>system_group</code> resource group). WarehousePG starts with the query that consumes the largest amount of memory. The query will terminate until the percentage of memory used falls below the specified percentage.</p></li></ul><h2 id="assigning-a-resource-group-to-a-role" tabindex="-1"><a id="topic17"></a>Assigning a Resource Group to a Role <a class="header-anchor" href="#assigning-a-resource-group-to-a-role" aria-label="Permalink to &quot;&lt;a id=&quot;topic17&quot;&gt;&lt;/a&gt;Assigning a Resource Group to a Role&quot;">​</a></h2><p>You assign a resource group to a database role using the <code>RESOURCE GROUP</code> clause of the <a href="./../ref_guide/sql_commands/CREATE_ROLE.html">CREATE ROLE</a> or <a href="./../ref_guide/sql_commands/ALTER_ROLE.html">ALTER ROLE</a> commands. If you do not specify a resource group for a role, the role is assigned the default group for the role&#39;s capability. <code>SUPERUSER</code> roles are assigned the <code>admin_group</code>, non-admin roles are assigned the group named <code>default_group</code>.</p><p>Use the <code>ALTER ROLE</code> or <code>CREATE ROLE</code> commands to assign a resource group to a role. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ALTER ROLE bill RESOURCE GROUP rg_light;</span></span>
<span class="line"><span>CREATE ROLE mary RESOURCE GROUP exec;</span></span></code></pre></div><p>You can assign a resource group to one or more roles. If you have defined a role hierarchy, assigning a resource group to a parent role does not propagate down to the members of that role group.</p><p>If you wish to remove a resource group assignment from a role and assign the role the default group, change the role&#39;s group name assignment to <code>NONE</code>. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ALTER ROLE mary RESOURCE GROUP NONE;</span></span></code></pre></div><h2 id="monitoring-resource-group-status" tabindex="-1"><a id="topic22"></a>Monitoring Resource Group Status <a class="header-anchor" href="#monitoring-resource-group-status" aria-label="Permalink to &quot;&lt;a id=&quot;topic22&quot;&gt;&lt;/a&gt;Monitoring Resource Group Status&quot;">​</a></h2><p>Monitoring the status of your resource groups and queries may involve the following tasks.</p><h3 id="viewing-resource-group-limits" tabindex="-1"><a id="topic221"></a>Viewing Resource Group Limits <a class="header-anchor" href="#viewing-resource-group-limits" aria-label="Permalink to &quot;&lt;a id=&quot;topic221&quot;&gt;&lt;/a&gt;Viewing Resource Group Limits&quot;">​</a></h3><p>The <a href="./../ref_guide/system_catalogs/catalog_ref-views.html#gp_resgroup_config">gp_resgroup_config</a> <code>gp_toolkit</code> system view displays the current limits for a resource group. To view the limits of all resource groups:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT * FROM gp_toolkit.gp_resgroup_config;</span></span></code></pre></div><h3 id="viewing-resource-group-query-status" tabindex="-1"><a id="topic23"></a>Viewing Resource Group Query Status <a class="header-anchor" href="#viewing-resource-group-query-status" aria-label="Permalink to &quot;&lt;a id=&quot;topic23&quot;&gt;&lt;/a&gt;Viewing Resource Group Query Status&quot;">​</a></h3><p>The <a href="./../ref_guide/system_catalogs/catalog_ref-views.html#gp_resgroup_status">gp_resgroup_status</a> <code>gp_toolkit</code> system view enables you to view the status and activity of a resource group. The view displays the number of running and queued transactions. To view this information:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT * FROM gp_toolkit.gp_resgroup_status;</span></span></code></pre></div><h3 id="viewing-resource-group-memory-usage-per-host" tabindex="-1"><a id="topic23a"></a>Viewing Resource Group Memory Usage Per Host <a class="header-anchor" href="#viewing-resource-group-memory-usage-per-host" aria-label="Permalink to &quot;&lt;a id=&quot;topic23a&quot;&gt;&lt;/a&gt;Viewing Resource Group Memory Usage Per Host&quot;">​</a></h3><p>The <a href="./../ref_guide/system_catalogs/catalog_ref-views.html#gp_resgroup_status_per_host">gp_resgroup_status_per_host</a> <code>gp_toolkit</code> system view enables you to view the real-time memory usage of a resource group on a per-host basis. To view this information:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT * FROM gp_toolkit.gp_resgroup_status_per_host;</span></span></code></pre></div><h3 id="viewing-the-resource-group-assigned-to-a-role" tabindex="-1"><a id="topic25"></a>Viewing the Resource Group Assigned to a Role <a class="header-anchor" href="#viewing-the-resource-group-assigned-to-a-role" aria-label="Permalink to &quot;&lt;a id=&quot;topic25&quot;&gt;&lt;/a&gt;Viewing the Resource Group Assigned to a Role&quot;">​</a></h3><p>To view the resource group-to-role assignments, perform the following query on the <a href="./../ref_guide/system_catalogs/pg_roles.html">pg_roles</a> and <a href="./../ref_guide/system_catalogs/pg_resgroup.html">pg_resgroup</a> system catalog tables:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT rolname, rsgname FROM pg_roles, pg_resgroup</span></span>
<span class="line"><span>WHERE pg_roles.rolresgroup=pg_resgroup.oid;</span></span></code></pre></div><h3 id="viewing-resource-group-disk-i-o-usage-per-host" tabindex="-1"><a id="topic25"></a>Viewing Resource Group Disk I/O Usage Per Host <a class="header-anchor" href="#viewing-resource-group-disk-i-o-usage-per-host" aria-label="Permalink to &quot;&lt;a id=&quot;topic25&quot;&gt;&lt;/a&gt;Viewing Resource Group Disk I/O Usage Per Host&quot;">​</a></h3><p>The <a href="./../ref_guide/system_catalogs/catalog_ref-views.html#gp_resgroup_iostats_per_host">gp_resgroup_iostats_per_host</a> <code>gp_toolkit</code> system view enables you to view the real-time disk I/O usage of a resource group on a per-host basis. To view this information:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT * FROM gp_toolkit.gp_resgroup_iostats_per_host;</span></span></code></pre></div><h3 id="viewing-a-resource-group-s-running-and-pending-queries" tabindex="-1"><a id="topic252525"></a>Viewing a Resource Group&#39;s Running and Pending Queries <a class="header-anchor" href="#viewing-a-resource-group-s-running-and-pending-queries" aria-label="Permalink to &quot;&lt;a id=&quot;topic252525&quot;&gt;&lt;/a&gt;Viewing a Resource Group&#39;s Running and Pending Queries&quot;">​</a></h3><p>To view a resource group&#39;s running queries, pending queries, and how long the pending queries have been queued, examine the <a href="./../ref_guide/system_catalogs/catalog_ref-views.html#pg_stat_activity">pg_stat_activity</a> system catalog table:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT query, rsgname,wait_event_type, wait_event </span></span>
<span class="line"><span>FROM pg_stat_activity;</span></span></code></pre></div><p><code>pg_stat_activity</code> displays information about the user/role that initiated a query. A query that uses an external component such as PL/Container is composed of two parts: the query operator that runs in WarehousePG and the UDF that runs in a PL/Container instance. WarehousePG processes the query operators under the resource group assigned to the role that initiated the query. A UDF running in a PL/Container instance runs under the resource group assigned to the PL/Container runtime. The latter is not represented in the <code>pg_stat_activity</code> view; WarehousePG does not have any insight into how external components such as PL/Container manage memory in running instances.</p><h3 id="cancelling-a-running-or-queued-transaction-in-a-resource-group" tabindex="-1"><a id="topic27"></a>Cancelling a Running or Queued Transaction in a Resource Group <a class="header-anchor" href="#cancelling-a-running-or-queued-transaction-in-a-resource-group" aria-label="Permalink to &quot;&lt;a id=&quot;topic27&quot;&gt;&lt;/a&gt;Cancelling a Running or Queued Transaction in a Resource Group&quot;">​</a></h3><p>There may be cases when you want to cancel a running or queued transaction in a resource group. For example, you may want to remove a query that is waiting in the resource group queue but has not yet been run. Or, you may want to stop a running query that is taking too long to run, or one that is sitting idle in a transaction and taking up resource group transaction slots that are needed by other users.</p><p>By default, transactions can remain queued in a resource group indefinitely. If you want WarehousePG to cancel a queued transaction after a specific amount of time, set the server configuration parameter <a href="./../ref_guide/config_params/guc-list.html#gp_resource_group_queuing_timeout">gp_resource_group_queuing_timeout</a>. When this parameter is set to a value (milliseconds) greater than 0, WarehousePG cancels any queued transaction when it has waited longer than the configured timeout.</p><p>To manually cancel a running or queued transaction, you must first determine the process id (pid) associated with the transaction. Once you have obtained the process id, you can invoke <code>pg_cancel_backend()</code> to end that process, as shown below.</p><p>For example, to view the process information associated with all statements currently active or waiting in all resource groups, run the following query. If the query returns no results, then there are no running or queued transactions in any resource group.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT rolname, g.rsgname, pid, waiting, state, query, datname </span></span>
<span class="line"><span>FROM pg_roles, gp_toolkit.gp_resgroup_status g, pg_stat_activity </span></span>
<span class="line"><span>WHERE pg_roles.rolresgroup=g.groupid</span></span>
<span class="line"><span>AND pg_stat_activity.usename=pg_roles.rolname;</span></span></code></pre></div><p>Sample partial query output:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span> rolname | rsgname  | pid     | waiting | state  |          query           | datname </span></span>
<span class="line"><span>---------+----------+---------+---------+--------+------------------------ -+---------</span></span>
<span class="line"><span>  sammy  | rg_light |  31861  |    f    | idle   | SELECT * FROM mytesttbl; | testdb</span></span>
<span class="line"><span>  billy  | rg_light |  31905  |    t    | active | SELECT * FROM topten;    | testdb</span></span></code></pre></div><p>Use this output to identify the process id (<code>pid</code>) of the transaction you want to cancel, and then cancel the process. For example, to cancel the pending query identified in the sample output above:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT pg_cancel_backend(31905);</span></span></code></pre></div><p>You can provide an optional message in a second argument to <code>pg_cancel_backend()</code> to indicate to the user why the process was cancelled.</p><blockquote><p><strong>Note</strong> Do not use an operating system <code>KILL</code> command to cancel any WarehousePG process.</p></blockquote><h2 id="moving-a-query-to-a-different-resource-group" tabindex="-1"><a id="moverg"></a>Moving a Query to a Different Resource Group <a class="header-anchor" href="#moving-a-query-to-a-different-resource-group" aria-label="Permalink to &quot;&lt;a id=&quot;moverg&quot;&gt;&lt;/a&gt;Moving a Query to a Different Resource Group&quot;">​</a></h2><p>A user with WarehousePG superuser privileges can run the <code>gp_toolkit.pg_resgroup_move_query()</code> function to move a running query from one resource group to another, without stopping the query. Use this function to expedite a long-running query by moving it to a resource group with a higher resource allotment or availability.</p><blockquote><p><strong>Note</strong> You can move only an active or running query to a new resource group. You cannot move a queued or pending query that is in an idle state due to concurrency or memory limits.</p></blockquote><p><code>pg_resgroup_move_query()</code> requires the process id (pid) of the running query, as well as the name of the resource group to which you want to move the query. The signature of the function follows:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pg_resgroup_move_query( pid int4, group_name text );</span></span></code></pre></div><p>You can obtain the pid of a running query from the <code>pg_stat_activity</code> system view as described in <a href="#topic27">Cancelling a Running or Queued Transaction in a Resource Group</a>. Use the <code>gp_toolkit.gp_resgroup_status</code> view to list the name, id, and status of each resource group.</p><p>When you invoke <code>pg_resgroup_move_query()</code>, the query is subject to the limits configured for the destination resource group:</p><ul><li>If the group has already reached its concurrent task limit, WarehousePG queues the query until a slot opens or for <code>gp_resource_group_queuing_timeout</code> milliseconds if set.</li><li>If the group has a free slot, <code>pg_resgroup_move_query()</code> tries to give slot control away to the target process for up to <code>gp_resource_group_move_timeout</code> milliseconds. If target process can&#39;t handle movement request until <code>gp_resource_group_queuing_timeout</code> exceeds, WarehousePG returns the error: <code>target process failed to move to a new group</code>.</li><li>If <code>pg_resgroup_move_query()</code> was cancelled, but target process already got all slot controls, segment&#39;s processes will not be moved to new group, and target process will hold the slot. Such inconsistent state will be fixed by the end of transaction or by any next command dispatched by target process inside same transaction.</li><li>If the destination resource group does not have enough memory available to service the query&#39;s current memory requirements, WarehousePG returns the error: <code>group &lt;group_name&gt; doesn&#39;t have enough memory ...</code>. In this situation, you may choose to increase the group shared memory allotted to the destination resource group, or perhaps wait a period of time for running queries to complete and then invoke the function again.</li></ul><p>After WarehousePG moves the query, there is no way to guarantee that a query currently running in the destination resource group does not exceed the group memory quota. In this situation, one or more running queries in the destination group may fail, including the moved query. Reserve enough resource group global shared memory to minimize the potential for this scenario to occur.</p><p><code>pg_resgroup_move_query()</code> moves only the specified query to the destination resource group. WarehousePG assigns subsequent queries that you submit in the session to the original resource group.</p><h2 id="frequently-asked-questions" tabindex="-1"><a id="topic777999"></a>Frequently Asked Questions <a class="header-anchor" href="#frequently-asked-questions" aria-label="Permalink to &quot;&lt;a id=&quot;topic777999&quot;&gt;&lt;/a&gt;Frequently Asked Questions&quot;">​</a></h2><p><strong>Why is CPU usage lower than the <code>CPU_MAX_PERCENT</code> configured for the resource group?</strong></p><p>You may run into this situation when a low number of queries and slices are running in the resource group, and these processes are not utilizing all of the cores on the system.</p><p>**My resource group has a <code>CPU_WEIGHT</code> equivalent to 40%. Why is the CPU usage never reaching this limit?</p><p>The value of <code>CPU_MAX_PERCENT</code> might be lower than 40, hence it might be limiting the CPU usage even with idle resources.</p><p><strong>Why is the number of running transactions lower than the <code>CONCURRENCY</code> limit configured for the resource group?</strong></p><p>WarehousePG considers memory availability before running a transaction, and will queue the transaction if there is not enough memory available to serve it. If you use <code>ALTER RESOURCE GROUP</code> to increase the <code>CONCURRENCY</code> limit for a resource group but do not also adjust memory limits, currently running transactions may be consuming all allotted memory resources for the group. When in this state, WarehousePG queues subsequent transactions in the resource group.</p><p><strong>Why is the number of running transactions in the resource group higher than the configured <code>CONCURRENCY</code> limit?</strong></p><p>This behaviour is expected. There are several reasons why this may happen:</p><ul><li>Resource groups do not enforce resource restrictions on <code>SET</code>, <code>RESET</code> and <code>SHOW</code> commands</li><li>The server configuration parameter <code>gp_resource_group_bypass</code> disables the concurrent transaction limit for the resource group so a query can run immediately.</li><li>If the server configuration parameter <code>gp_resource_group_bypass_catalog_query</code> is set to true (the default), all queries that read exclusively from system catalogs, or queries that contain in their query text <code>pg_catalog</code> schema tables only will not enforce the limits of the resource group.</li><li>Queries whose plan cost is less than the limit <code>MIN_COST</code> will be automatically unassigned from their resource group and will not enforce any of the limits set for this.</li></ul><p><strong>Why did my query return a &quot;memory limit reached&quot; error?</strong></p><p>WarehousePG automatically adjusts transaction and group memory to the new settings when you use <code>ALTER RESOURCE GROUP</code> to change a resource group&#39;s memory and/or concurrency limits. An &quot;out of memory&quot; error may occur if you recently altered resource group attributes and there is no longer a sufficient amount of memory available for a currently running query.</p><p><strong>My query cannot run due to insufficient memory, resulting in memory leak Out of Memory (OOM).</strong></p><p>First, ensure that the resource group is allocating enough memory required by the query by tuning resource group parameters such as <code>CONCURRENCY</code> and <code>MEMORY_QUOTA</code>. Analyze the type of query, whether there will be a lot of intermediate results using memory. If it does exist, you can set a reasonable <code>gp_resgroup_memory_query_fixed_mem</code> to allocate more memory at the session level for this specific query.</p><p><strong>After a memory leak OOM the system has a high concurrent load</strong>.</p><p>When the system starts to clean up the sessions left over by the memory leak, the concurrent load of the system is high at this time, and the OOM error message may reappear. Due to the current design, we cannot expedite the cleanup process of the Runaway Session. The solution to this problem is to adjust the <code>runaway_detector_activation_percent</code> to 0.85 or 0.8, or even lower, in order to increase the available memory of the segment host.</p><p><strong>Some transaction requests only run during a certain period of time, and do not run at other times.</strong></p><p>You may change the configuration of resource groups can be changed dynamically at regular intervals to match the requirements of your workload, and customize resource allocation at different times to achieve higher efficiency. For example, change the configuration of resources within a group, add or delete resource groups.</p><p><strong>After upgrading WarehousePG, the performance seems to be degraded.</strong></p><p>There are many factors that can affect performance degradation. A possible cause is that after upgrading to WarehousePG 7, SWAP is enabled by default and hence affecting your performance. We recommend disabling SWAP and use RAM memory instead, in order to improve running speed and efficiency. If your memory configuration is sufficient, there is no need to use SWAP space. If you decide to use SWAP, be sure you understand how it takes part in the calculation of memory management allocation. See <a href="./wlmgmt_intro.html">WarehousePG Memory Overview</a> for more information.</p>`,182)]))}const g=o(r,[["render",i]]);export{h as __pageData,g as default};
