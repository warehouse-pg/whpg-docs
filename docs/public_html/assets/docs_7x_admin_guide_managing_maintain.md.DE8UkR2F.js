import{_ as a,c as t,o,ag as i}from"./chunks/framework.Ds6Eueu6.js";const h=JSON.parse('{"title":"Routine System Maintenance Tasks","description":"","frontmatter":{},"headers":[],"relativePath":"docs/7x/admin_guide/managing/maintain.md","filePath":"docs/7x/admin_guide/managing/maintain.md"}'),n={name:"docs/7x/admin_guide/managing/maintain.md"};function s(r,e,c,l,d,p){return o(),t("div",null,e[0]||(e[0]=[i(`<h1 id="routine-system-maintenance-tasks" tabindex="-1">Routine System Maintenance Tasks <a class="header-anchor" href="#routine-system-maintenance-tasks" aria-label="Permalink to &quot;Routine System Maintenance Tasks&quot;">​</a></h1><hr><p>To keep a WarehousePG cluster running efficiently, the database must be regularly cleared of expired data and the table statistics must be updated so that the query optimizer has accurate information.</p><p>WarehousePG requires that certain tasks be performed regularly to achieve optimal performance. The tasks discussed here are required, but database administrators can automate them using standard UNIX tools such as <code>cron</code> scripts. An administrator sets up the appropriate scripts and checks that they ran successfully. See <a href="./../monitoring/monitoring.html">Recommended Monitoring and Maintenance Tasks</a> for additional suggested maintenance activities you can implement to keep your WarehousePG cluster running optimally.</p><p><strong>Parent topic:</strong> <a href="./../managing/managing.html">Managing a WarehousePG cluster</a></p><h2 id="routine-vacuum-and-analyze" tabindex="-1"><a id="topic2"></a>Routine Vacuum and Analyze <a class="header-anchor" href="#routine-vacuum-and-analyze" aria-label="Permalink to &quot;&lt;a id=&quot;topic2&quot;&gt;&lt;/a&gt;Routine Vacuum and Analyze&quot;">​</a></h2><p>The design of the MVCC transaction concurrency model used in WarehousePG means that deleted or updated data rows still occupy physical space on disk even though they are not visible to new transactions. If your database has many updates and deletes, many expired rows exist and the space they use must be reclaimed with the <code>VACUUM</code> command. The <code>VACUUM</code> command also collects table-level statistics, such as numbers of rows and pages, so it is also necessary to vacuum append-optimized tables, even when there is no space to reclaim from updated or deleted rows.</p><p>Vacuuming an append-optimized table follows a different process than vacuuming heap tables. On each segment, a new segment file is created and visible rows are copied into it from the current segment. When the segment file has been copied, the original is scheduled to be dropped and the new segment file is made available. This requires sufficient available disk space for a copy of the visible rows until the original segment file is dropped.</p><p>If the ratio of hidden rows to total rows in a segment file is less than a threshold value (10, by default), the segment file is not compacted. The threshold value can be configured with the <code>gp_appendonly_compaction_threshold</code> server configuration parameter. <code>VACUUM FULL</code> ignores the value of <code>gp_appendonly_compaction_threshold</code> and rewrites the segment file regardless of the ratio.</p><p>You can use the <code>__gp_aovisimap_compaction_info()</code> function in the <em>gp_toolkit</em> schema to investigate the effectiveness of a VACUUM operation on append-optimized tables.</p><p>For information about the <code>__gp_aovisimap_compaction_info()</code> function, see <a href="./../../ref_guide/gp_toolkit.html#topic8">Checking Append-Optimized Tables</a>.</p><p><code>VACUUM</code> can be deactivated for append-optimized tables using the <code>gp_appendonly_compaction</code> server configuration parameter.</p><p>For details about vacuuming a database, see <a href="./../dml.html">Vacuuming the Database</a>.</p><p>For information about the <code>gp_appendonly_compaction_threshold</code> server configuration parameter and the <code>VACUUM</code> command, see the <em>WarehousePG Reference Guide</em>.</p><h3 id="transaction-id-management" tabindex="-1"><a id="topic3"></a>Transaction ID Management <a class="header-anchor" href="#transaction-id-management" aria-label="Permalink to &quot;&lt;a id=&quot;topic3&quot;&gt;&lt;/a&gt;Transaction ID Management&quot;">​</a></h3><p>WarehousePG&#39;s MVCC transaction semantics depend on comparing transaction ID (XID) numbers to determine visibility to other transactions. Transaction ID numbers are compared using modulo 232 arithmetic, so a WarehousePG cluster that runs more than about two billion transactions can experience transaction ID wraparound, where past transactions appear to be in the future. This means past transactions&#39; outputs become invisible. Therefore, it is necessary to <code>VACUUM</code> every table in every database at least once per two billion transactions.</p><p>WarehousePG assigns XID values only to transactions that involve DDL or DML operations, which are typically the only transactions that require an XID.</p><blockquote><p><strong>Important</strong> WarehousePG monitors transaction IDs. If you do not vacuum the database regularly, WarehousePG will generate a warning and error.</p></blockquote><p>WarehousePG issues the following warning when a significant portion of the transaction IDs are no longer available and before transaction ID wraparound occurs:</p><p><code>WARNING: database &quot;database_name&quot; must be vacuumed within *number\\_of\\_transactions* transactions</code></p><p>When the warning is issued, a <code>VACUUM</code> operation is required. If a <code>VACUUM</code> operation is not performed, WarehousePG stops creating transactions when it reaches a limit prior to when transaction ID wraparound occurs. WarehousePG issues this error when it stops creating transactions to avoid possible data loss:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>FATAL: database is not accepting commands to avoid </span></span>
<span class="line"><span>wraparound data loss in database &quot;database_name&quot;</span></span></code></pre></div><p>The WarehousePG configuration parameter <code>xid_warn_limit</code> controls when the warning is displayed. The parameter <code>xid_stop_limit</code> controls when WarehousePG stops creating transactions.</p><h4 id="recovering-from-a-transaction-id-limit-error" tabindex="-1"><a id="np160654"></a>Recovering from a Transaction ID Limit Error <a class="header-anchor" href="#recovering-from-a-transaction-id-limit-error" aria-label="Permalink to &quot;&lt;a id=&quot;np160654&quot;&gt;&lt;/a&gt;Recovering from a Transaction ID Limit Error&quot;">​</a></h4><p>When WarehousePG reaches the <code>xid_stop_limit</code> transaction ID limit due to infrequent <code>VACUUM</code> maintenance, it becomes unresponsive. To recover from this situation, perform the following steps as database administrator:</p><ol><li>Shut down WarehousePG.</li><li>Temporarily lower the <code>xid_stop_limit</code> by 10,000,000.</li><li>Start WarehousePG.</li><li>Run <code>VACUUM FREEZE</code> on all affected databases.</li><li>Reset the <code>xid_stop_limit</code> to its original value.</li><li>Restart WarehousePG.</li></ol><p>For information about the configuration parameters, see the <em>WarehousePG Reference Guide</em>.</p><p>For information about transaction ID wraparound see the <a href="https://www.postgresql.org/docs/12/index.html" target="_blank" rel="noreferrer">PostgreSQL documentation</a>.</p><h3 id="system-catalog-maintenance" tabindex="-1"><a id="topic4"></a>System Catalog Maintenance <a class="header-anchor" href="#system-catalog-maintenance" aria-label="Permalink to &quot;&lt;a id=&quot;topic4&quot;&gt;&lt;/a&gt;System Catalog Maintenance&quot;">​</a></h3><p>Numerous database updates with <code>CREATE</code> and <code>DROP</code> commands increase the system catalog size and affect system performance. For example, running many <code>DROP TABLE</code> statements degrades the overall system performance due to excessive data scanning during metadata operations on catalog tables. The performance loss occurs between thousands to tens of thousands of <code>DROP TABLE</code> statements, depending on the system.</p><p>You should run a system catalog maintenance procedure regularly to reclaim the space occupied by deleted objects. If a regular procedure has not been run for a long time, you may need to run a more intensive procedure to clear the system catalog. This topic describes both procedures.</p><h4 id="regular-system-catalog-maintenance" tabindex="-1"><a id="topic5"></a>Regular System Catalog Maintenance <a class="header-anchor" href="#regular-system-catalog-maintenance" aria-label="Permalink to &quot;&lt;a id=&quot;topic5&quot;&gt;&lt;/a&gt;Regular System Catalog Maintenance&quot;">​</a></h4><p>It is recommended that you periodically run <code>REINDEX</code> and <code>VACUUM</code> on the system catalog to clear the space that deleted objects occupy in the system indexes and tables. If regular database operations include numerous <code>DROP</code> statements, it is safe and appropriate to run a system catalog maintenance procedure with <code>VACUUM</code> daily at off-peak hours. You can do this while the system is available.</p><p>These are WarehousePG cluster catalog maintenance steps.</p><ol><li><p>Perform a <code>REINDEX</code> on the system catalog tables to rebuild the system catalog indexes. This removes bloat in the indexes and improves <code>VACUUM</code> performance.</p><blockquote><p><strong>Note</strong> <code>REINDEX</code> causes locking of system catalog tables, which could affect currently running queries. To avoid disrupting ongoing business operations, schedule the <code>REINDEX</code> operation during a period of low activity.</p></blockquote></li><li><p>Perform a <code>VACUUM</code> on the system catalog tables.</p></li><li><p>Perform an <code>ANALYZE</code> on the system catalog tables to update the catalog table statistics.</p></li></ol><p>This example script performs a <code>REINDEX</code>, <code>VACUUM</code>, and <code>ANALYZE</code> of a WarehousePG cluster catalog. In the script, replace <code>&lt;database-name&gt;</code> with a database name.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>#!/bin/bash</span></span>
<span class="line"><span>DBNAME=&quot;&lt;database-name&gt;&quot;</span></span>
<span class="line"><span>SYSTABLES=&quot;&#39; pg_catalog.&#39; || relname || &#39;;&#39; FROM pg_class a, pg_namespace b </span></span>
<span class="line"><span>WHERE a.relnamespace=b.oid AND b.nspname=&#39;pg_catalog&#39; AND a.relkind=&#39;r&#39;&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>reindexdb --system -d $DBNAME</span></span>
<span class="line"><span>psql -tc &quot;SELECT &#39;VACUUM&#39; || $SYSTABLES&quot; $DBNAME | psql -a $DBNAME</span></span>
<span class="line"><span>analyzedb -as pg_catalog -d $DBNAME</span></span></code></pre></div><blockquote><p><strong>Note</strong> If you are performing catalog maintenance during a maintenance period and you need to stop a process due to time constraints, run the WarehousePG function <code>pg_cancel_backend(&lt;PID&gt;)</code> to safely stop the WarehousePG process.</p></blockquote><h4 id="intensive-system-catalog-maintenance" tabindex="-1"><a id="topic6"></a>Intensive System Catalog Maintenance <a class="header-anchor" href="#intensive-system-catalog-maintenance" aria-label="Permalink to &quot;&lt;a id=&quot;topic6&quot;&gt;&lt;/a&gt;Intensive System Catalog Maintenance&quot;">​</a></h4><p>If system catalog maintenance has not been performed in a long time, the catalog can become bloated with dead space; this causes excessively long wait times for simple metadata operations. A wait of more than two seconds to list user tables, such as with the <code>\\d</code> metacommand from within <code>psql</code>, is an indication of catalog bloat.</p><p>If you see indications of system catalog bloat, you must perform an intensive system catalog maintenance procedure with <code>VACUUM FULL</code> during a scheduled downtime period. During this period, stop all catalog activity on the system; the <code>VACUUM FULL</code> system catalog maintenance procedure takes exclusive locks against the system catalog.</p><p>Running regular system catalog maintenance procedures can prevent the need for this more costly procedure.</p><p>These are steps for intensive system catalog maintenance.</p><ol><li>Stop all catalog activity on the WarehousePG cluster.</li><li>Perform a <code>REINDEX</code> on the system catalog tables to rebuild the system catalog indexes. This removes bloat in the indexes and improves <code>VACUUM</code> performance.</li><li>Perform a <code>VACUUM FULL</code> on the system catalog tables. See the following Note.</li><li>Perform an <code>ANALYZE</code> on the system catalog tables to update the catalog table statistics.</li></ol><blockquote><p><strong>Note</strong> The system catalog table <code>pg_attribute</code> is usually the largest catalog table. If the <code>pg_attribute</code> table is significantly bloated, a <code>VACUUM FULL</code> operation on the table might require a significant amount of time and might need to be performed separately. The presence of both of these conditions indicate a significantly bloated <code>pg_attribute</code> table that might require a long <code>VACUUM FULL</code> time:</p></blockquote><ul><li>The <code>pg_attribute</code> table contains a large number of records.</li><li>The diagnostic message for <code>pg_attribute</code> is <code>significant amount of bloat</code> in the <code>gp_toolkit.gp_bloat_diag</code> view.</li></ul><h3 id="vacuum-and-analyze-for-query-optimization" tabindex="-1"><a id="topic7"></a>Vacuum and Analyze for Query Optimization <a class="header-anchor" href="#vacuum-and-analyze-for-query-optimization" aria-label="Permalink to &quot;&lt;a id=&quot;topic7&quot;&gt;&lt;/a&gt;Vacuum and Analyze for Query Optimization&quot;">​</a></h3><p>WarehousePG uses a cost-based query optimizer that relies on database statistics. Accurate statistics allow the query optimizer to better estimate selectivity and the number of rows that a query operation retrieves. These estimates help it choose the most efficient query plan. The <code>ANALYZE</code> command collects column-level statistics for the query optimizer.</p><p>You can run both <code>VACUUM</code> and <code>ANALYZE</code> operations in the same command. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>=# VACUUM ANALYZE mytable;</span></span></code></pre></div><p>Running the VACUUM ANALYZE command might produce incorrect statistics when the command is run on a table with a significant amount of bloat (a significant amount of table disk space is occupied by deleted or obsolete rows). For large tables, the <code>ANALYZE</code> command calculates statistics from a random sample of rows. It estimates the number rows in the table by multiplying the average number of rows per page in the sample by the number of actual pages in the table. If the sample contains many empty pages, the estimated row count can be inaccurate.</p><p>For a table, you can view information about the amount of unused disk space (space that is occupied by deleted or obsolete rows) in the <em>gp_toolkit</em> view <em>gp_bloat_diag</em>. If the <code>bdidiag</code> column for a table contains the value <code>significant amount of bloat suspected</code>, a significant amount of table disk space consists of unused space. Entries are added to the <em>gp_bloat_diag</em> view after a table has been vacuumed.</p><p>To remove unused disk space from the table, you can run the command VACUUM FULL on the table. Due to table lock requirements, VACUUM FULL might not be possible until a maintenance period.</p><p>As a temporary workaround, run ANALYZE to compute column statistics and then run VACUUM on the table to generate an accurate row count. This example runs ANALYZE and then VACUUM on the <em>cust_info</em> table.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ANALYZE cust_info;</span></span>
<span class="line"><span>VACUUM cust_info;</span></span></code></pre></div><blockquote><p><strong>Important</strong> If you intend to run queries on partitioned tables with GPORCA enabled (the default), you must collect statistics on the root partitioned table with the ANALYZE command. For information about GPORCA, see <a href="./../query/topics/query-piv-opt-overview.html">Overview of GPORCA</a>.</p></blockquote><blockquote><p><strong>Note</strong> You can use the WarehousePG utility analyzedb to update table statistics. Tables can be analyzed concurrently. For append optimized tables, analyzedb updates statistics only if the statistics are not current. See the <a href="./../../utility_guide/ref/analyzedb.html">analyzedb</a> utility.</p></blockquote><h2 id="routine-reindexing" tabindex="-1"><a id="topic8"></a>Routine Reindexing <a class="header-anchor" href="#routine-reindexing" aria-label="Permalink to &quot;&lt;a id=&quot;topic8&quot;&gt;&lt;/a&gt;Routine Reindexing&quot;">​</a></h2><p>For B-tree indexes, a freshly-constructed index is slightly faster to access than one that has been updated many times because logically adjacent pages are usually also physically adjacent in a newly built index. Reindexing older indexes periodically can improve access speed. If all but a few index keys on a page have been deleted, there will be wasted space on the index page. A reindex will reclaim that wasted space. In WarehousePG it is often faster to drop an index (<code>DROP INDEX</code>) and then recreate it (<code>CREATE INDEX</code>) than it is to use the <code>REINDEX</code> command.</p><p>For table columns with indexes, some operations such as bulk updates or inserts to the table might perform more slowly because of the updates to the indexes. To enhance performance of bulk operations on tables with indexes, you can drop the indexes, perform the bulk operation, and then re-create the index.</p><h2 id="managing-warehousepg-log-files" tabindex="-1"><a id="topic9"></a>Managing WarehousePG Log Files <a class="header-anchor" href="#managing-warehousepg-log-files" aria-label="Permalink to &quot;&lt;a id=&quot;topic9&quot;&gt;&lt;/a&gt;Managing WarehousePG Log Files&quot;">​</a></h2><ul><li><a href="#topic10">Database Server Log Files</a></li><li><a href="#topic11">Management Utility Log Files</a></li></ul><h3 id="database-server-log-files" tabindex="-1"><a id="topic10"></a>Database Server Log Files <a class="header-anchor" href="#database-server-log-files" aria-label="Permalink to &quot;&lt;a id=&quot;topic10&quot;&gt;&lt;/a&gt;Database Server Log Files&quot;">​</a></h3><p>WarehousePG log output tends to be voluminous, especially at higher debug levels, and you do not need to save it indefinitely. Administrators should purge older log files periodically.</p><p>WarehousePG by default has log file rotation enabled for the coordinator and segment database logs. Log files are created in the <code>log</code> subdirectory of the coordinator and each segment data directory using the following naming convention: <code>gpdb-*YYYY*-*MM*-*DD_hhmmss*.csv</code>. Administrators need to implement scripts or programs to periodically clean up old log files in the <code>log</code> directory of the coordinator and each segment instance.</p><p>Log rotation can be triggered by the size of the current log file or the age of the current log file. The <code>log_rotation_size</code> configuration parameter sets the size of an individual log file that triggers log rotation. When the log file size is equal to or greater than the specified size, the file is closed and a new log file is created. The <code>log_rotation_size</code> value is specified in kilobytes. The default is 1048576 kilobytes, or 1GB. If <code>log_rotation_size</code> is set to 0, size-based rotation is deactivated.</p><p>The <code>log_rotation_age</code> configuration parameter specifies the age of a log file that triggers rotation. When the specified amount of time has elapsed since the log file was created, the file is closed and a new log file is created. The default <code>log_rotation_age</code>, 1d, creates a new log file 24 hours after the current log file was created. If <code>log_rotation_age</code> is set to 0, time-based rotation is deactivated.</p><p>For information about viewing the database server log files, see <a href="./monitor.html">Viewing the Database Server Log Files</a>.</p><h3 id="management-utility-log-files" tabindex="-1"><a id="topic11"></a>Management Utility Log Files <a class="header-anchor" href="#management-utility-log-files" aria-label="Permalink to &quot;&lt;a id=&quot;topic11&quot;&gt;&lt;/a&gt;Management Utility Log Files&quot;">​</a></h3><p>Log files for the WarehousePG management utilities are written to <code>~/gpAdminLogs</code> by default. The naming convention for management log files is:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>&lt;script_name&gt;_&lt;date&gt;.log</span></span></code></pre></div><p>The log entry format is:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>&lt;timestamp&gt;:&lt;utility&gt;:&lt;host&gt;:&lt;user&gt;:[INFO|WARN|FATAL]:&lt;message&gt;</span></span></code></pre></div><p>The log file for a particular utility execution is appended to its daily log file each time that utility is run.</p>`,74)]))}const m=a(n,[["render",s]]);export{h as __pageData,m as default};
