import{_ as t,c as a,o as s,ag as o}from"./chunks/framework.Ds6Eueu6.js";const m=JSON.parse('{"title":"Distribution and Skew","description":"","frontmatter":{},"headers":[],"relativePath":"docs/7x/admin_guide/distribution.md","filePath":"docs/7x/admin_guide/distribution.md"}'),i={name:"docs/7x/admin_guide/distribution.md"};function n(r,e,d,l,c,h){return s(),a("div",null,e[0]||(e[0]=[o(`<h1 id="distribution-and-skew" tabindex="-1">Distribution and Skew <a class="header-anchor" href="#distribution-and-skew" aria-label="Permalink to &quot;Distribution and Skew&quot;">​</a></h1><hr><p>WarehousePG relies on even distribution of data across segments.</p><p>In an MPP shared nothing environment, overall response time for a query is measured by the completion time for all segments. The system is only as fast as the slowest segment. If the data is skewed, segments with more data will take more time to complete, so every segment must have an approximately equal number of rows and perform approximately the same amount of processing. Poor performance and out of memory conditions may result if one segment has significantly more data to process than other segments.</p><p>Optimal distributions are critical when joining large tables together. To perform a join, matching rows must be located together on the same segment. If data is not distributed on the same join column, the rows needed from one of the tables are dynamically redistributed to the other segments. In some cases a broadcast motion, in which each segment sends its individual rows to all other segments, is performed rather than a redistribution motion, where each segment rehashes the data and sends the rows to the appropriate segments according to the hash key.</p><p><strong>Parent topic:</strong> <a href="./../admin_guide.html">WarehousePG Administrator Guide</a></p><h2 id="local-co-located-joins" tabindex="-1"><a id="topic1"></a>Local (Co-located) Joins <a class="header-anchor" href="#local-co-located-joins" aria-label="Permalink to &quot;&lt;a id=&quot;topic1&quot;&gt;&lt;/a&gt;Local \\(Co-located\\) Joins&quot;">​</a></h2><p>Using a hash distribution that evenly distributes table rows across all segments and results in local joins can provide substantial performance gains. When joined rows are on the same segment, much of the processing can be accomplished within the segment instance. These are called <em>local</em> or <em>co-located</em> joins. Local joins minimize data movement; each segment operates independently of the other segments, without network traffic or communications between segments.</p><p>To achieve local joins for large tables commonly joined together, distribute the tables on the same column. Local joins require that both sides of a join be distributed on the same columns (and in the same order) <em>and</em> that all columns in the distribution clause are used when joining tables. The distribution columns must also be the same data type—although some values with different data types may appear to have the same representation, they are stored differently and hash to different values, so they are stored on different segments.</p><h2 id="data-skew" tabindex="-1"><a id="topic2"></a>Data Skew <a class="header-anchor" href="#data-skew" aria-label="Permalink to &quot;&lt;a id=&quot;topic2&quot;&gt;&lt;/a&gt;Data Skew&quot;">​</a></h2><p>Data skew may be caused by uneven data distribution due to the wrong choice of distribution keys or single tuple table insert or copy operations. Present at the table level, data skew, is often the root cause of poor query performance and out of memory conditions. Skewed data affects scan (read) performance, but it also affects all other query execution operations, for instance, joins and group by operations.</p><p>It is very important to <em>validate</em> distributions to <em>ensure</em> that data is evenly distributed after the initial load. It is equally important to <em>continue</em> to validate distributions after incremental loads.</p><p>The following query shows the number of rows per segment as well as the variance from the minimum and maximum numbers of rows:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT &#39;Example Table&#39; AS &quot;Table Name&quot;, </span></span>
<span class="line"><span>    max(c) AS &quot;Max Seg Rows&quot;, min(c) AS &quot;Min Seg Rows&quot;, </span></span>
<span class="line"><span>    (max(c)-min(c))*100.0/max(c) AS &quot;Percentage Difference Between Max &amp; Min&quot; </span></span>
<span class="line"><span>FROM (SELECT count(*) c, gp_segment_id FROM facts GROUP BY 2) AS a;</span></span></code></pre></div><p>The <code>gp_toolkit</code> schema has two views that you can use to check for skew.</p><ul><li>The <code>gp_toolkit.gp_skew_coefficients</code> view shows data distribution skew by calculating the coefficient of variation (CV) for the data stored on each segment. The <code>skccoeff</code> column shows the coefficient of variation (CV), which is calculated as the standard deviation divided by the average. It takes into account both the average and variability around the average of a data series. The lower the value, the better. Higher values indicate greater data skew.</li><li>The <code>gp_toolkit.gp_skew_idle_fractions</code> view shows data distribution skew by calculating the percentage of the system that is idle during a table scan, which is an indicator of computational skew. The <code>siffraction</code> column shows the percentage of the system that is idle during a table scan. This is an indicator of uneven data distribution or query processing skew. For example, a value of 0.1 indicates 10% skew, a value of 0.5 indicates 50% skew, and so on. Tables that have more than10% skew should have their distribution policies evaluated.</li></ul><h3 id="considerations-for-replicated-tables" tabindex="-1"><a id="section_unk_dpf_kgb"></a>Considerations for Replicated Tables <a class="header-anchor" href="#considerations-for-replicated-tables" aria-label="Permalink to &quot;&lt;a id=&quot;section_unk_dpf_kgb&quot;&gt;&lt;/a&gt;Considerations for Replicated Tables&quot;">​</a></h3><p>When you create a replicated table (with the <code>CREATE TABLE</code> clause <code>DISTRIBUTED REPLICATED</code>), WarehousePG distributes every table row to every segment instance. Replicated table data is evenly distributed because every segment has the same rows. A query that uses the <code>gp_segment_id</code> system column on a replicated table to verify evenly distributed data, will fail because WarehousePG does not allow queries to reference replicated tables&#39; system columns.</p><h2 id="processing-skew" tabindex="-1"><a id="topic3"></a>Processing Skew <a class="header-anchor" href="#processing-skew" aria-label="Permalink to &quot;&lt;a id=&quot;topic3&quot;&gt;&lt;/a&gt;Processing Skew&quot;">​</a></h2><p>Processing skew results when a disproportionate amount of data flows to, and is processed by, one or a few segments. It is often the culprit behind WarehousePG performance and stability issues. It can happen with operations such join, sort, aggregation, and various OLAP operations. Processing skew happens in flight while a query is running and is not as easy to detect as data skew.</p><p>If single segments are failing, that is, not all segments on a host, it may be a processing skew issue. Identifying processing skew is currently a manual process. First look for spill files. If there is skew, but not enough to cause spill, it will not become a performance issue. If you determine skew exists, then find the query responsible for the skew.</p><p>The remedy for processing skew in almost all cases is to rewrite the query. Creating temporary tables can eliminate skew. Temporary tables can be randomly distributed to force a two-stage aggregation.</p>`,22)]))}const p=t(i,[["render",n]]);export{m as __pageData,p as default};
