import{_ as t,c as a,o as s,ag as i}from"./chunks/framework.Ds6Eueu6.js";const h=JSON.parse('{"title":"Common Causes of Performance Issues","description":"","frontmatter":{},"headers":[],"relativePath":"docs/7x/admin_guide/perf_issues.md","filePath":"docs/7x/admin_guide/perf_issues.md"}'),o={name:"docs/7x/admin_guide/perf_issues.md"};function r(n,e,l,d,u,c){return s(),a("div",null,e[0]||(e[0]=[i('<h1 id="common-causes-of-performance-issues" tabindex="-1">Common Causes of Performance Issues <a class="header-anchor" href="#common-causes-of-performance-issues" aria-label="Permalink to &quot;Common Causes of Performance Issues&quot;">​</a></h1><hr><p>This section explains the troubleshooting processes for common performance issues and potential solutions to these issues.</p><p><strong>Parent topic:</strong> <a href="./performance.thml.html">Managing Performance</a></p><h2 id="identifying-hardware-and-segment-failures" tabindex="-1"><a id="topic2"></a>Identifying Hardware and Segment Failures <a class="header-anchor" href="#identifying-hardware-and-segment-failures" aria-label="Permalink to &quot;&lt;a id=&quot;topic2&quot;&gt;&lt;/a&gt;Identifying Hardware and Segment Failures&quot;">​</a></h2><p>The performance of WarehousePG depends on the hardware and IT infrastructure on which it runs. WarehousePG is comprised of several servers (hosts) acting together as one cohesive system (array); as a first step in diagnosing performance problems, ensure that all WarehousePG segments are online. WarehousePG&#39;s performance will be as fast as the slowest host in the array. Problems with CPU utilization, memory management, I/O processing, or network load affect performance. Common hardware-related issues are:</p><ul><li><p><strong>Disk Failure</strong> – Although a single disk failure should not dramatically affect database performance if you are using RAID, disk resynchronization does consume resources on the host with failed disks. The <code>gpcheckperf</code> utility can help identify segment hosts that have disk I/O issues.</p></li><li><p><strong>Host Failure</strong> – When a host is offline, the segments on that host are nonoperational. This means other hosts in the array must perform twice their usual workload because they are running the primary segments and multiple mirrors. If mirrors are not enabled, service is interrupted. Service is temporarily interrupted to recover failed segments. The <code>gpstate</code> utility helps identify failed segments.</p></li><li><p><strong>Network Failure</strong> – Failure of a network interface card, a switch, or DNS server can bring down segments. If host names or IP addresses cannot be resolved within your WarehousePG cluster, these manifest themselves as interconnect errors in WarehousePG. The <code>gpcheckperf</code> utility helps identify segment hosts that have network issues.</p></li><li><p><strong>Disk Capacity</strong> – Disk capacity on your segment hosts should never exceed 70 percent full. WarehousePG needs some free space for runtime processing. To reclaim disk space that deleted rows occupy, run <code>VACUUM</code> after loads or updates.The <em>gp_toolkit</em> administrative schema has many views for checking the size of distributed database objects.</p><p>See the <em>WarehousePG Reference Guide</em> for information about checking database object sizes and disk space.</p></li></ul><h2 id="managing-workload" tabindex="-1"><a id="topic3"></a>Managing Workload <a class="header-anchor" href="#managing-workload" aria-label="Permalink to &quot;&lt;a id=&quot;topic3&quot;&gt;&lt;/a&gt;Managing Workload&quot;">​</a></h2><p>A database system has a limited CPU capacity, memory, and disk I/O resources. When multiple workloads compete for access to these resources, database performance degrades. Resource management maximizes system throughput while meeting varied business requirements. WarehousePG provides resource queues and resource groups to help you manage these system resources.</p><p>Resource queues and resource groups limit resource usage and the total number of concurrent queries running in the particular queue or group. By assigning database roles to the appropriate queue or group, administrators can control concurrent user queries and prevent system overload. For more information about resource queues and resource groups, including selecting the appropriate scheme for your WarehousePG environment, see <a href="./wlmgmt.html">Managing Resources</a>.</p><p>WarehousePG administrators should run maintenance workloads such as data loads and <code>VACUUM ANALYZE</code> operations after business hours. Do not compete with database users for system resources; perform administrative tasks at low-usage times.</p><h2 id="avoiding-contention" tabindex="-1"><a id="topic4"></a>Avoiding Contention <a class="header-anchor" href="#avoiding-contention" aria-label="Permalink to &quot;&lt;a id=&quot;topic4&quot;&gt;&lt;/a&gt;Avoiding Contention&quot;">​</a></h2><p>Contention arises when multiple users or workloads try to use the system in a conflicting way; for example, contention occurs when two transactions try to update a table simultaneously. A transaction that seeks a table-level or row-level lock will wait indefinitely for conflicting locks to be released. Applications should not hold transactions open for long periods of time, for example, while waiting for user input.</p><h2 id="maintaining-database-statistics" tabindex="-1"><a id="topic5"></a>Maintaining Database Statistics <a class="header-anchor" href="#maintaining-database-statistics" aria-label="Permalink to &quot;&lt;a id=&quot;topic5&quot;&gt;&lt;/a&gt;Maintaining Database Statistics&quot;">​</a></h2><p>WarehousePG uses a cost-based query optimizer that relies on database statistics. Accurate statistics allow the query optimizer to better estimate the number of rows retrieved by a query to choose the most efficient query plan. Without database statistics, the query optimizer cannot estimate how many records will be returned. The optimizer does not assume it has sufficient memory to perform certain operations such as aggregations, so it takes the most conservative action and does these operations by reading and writing from disk. This is significantly slower than doing them in memory. ANALYZE collects statistics about the database that the query optimizer needs.</p><blockquote><p><strong>Note</strong> When running an SQL command with GPORCA, WarehousePG issues a warning if the command performance could be improved by collecting statistics on a column or set of columns referenced by the command. The warning is issued on the command line and information is added to the WarehousePG log file. For information about collecting statistics on table columns, see the ANALYZE command in the <em>WarehousePG Reference Guide</em></p></blockquote><h3 id="identifying-statistics-problems-in-query-plans" tabindex="-1"><a id="topic6"></a>Identifying Statistics Problems in Query Plans <a class="header-anchor" href="#identifying-statistics-problems-in-query-plans" aria-label="Permalink to &quot;&lt;a id=&quot;topic6&quot;&gt;&lt;/a&gt;Identifying Statistics Problems in Query Plans&quot;">​</a></h3><p>Before you interpret a query plan for a query using EXPLAIN or <code>EXPLAIN ANALYZE</code>, familiarize yourself with the data to help identify possible statistics problems. Check the plan for the following indicators of inaccurate statistics:</p><ul><li><strong>Are the optimizer&#39;s estimates close to reality?</strong> Run <code>EXPLAIN ANALYZE</code> and see if the number of rows the optimizer estimated is close to the number of rows the query operation returned.</li><li><strong>Are selective predicates applied early in the plan?</strong> The most selective filters should be applied early in the plan so fewer rows move up the plan tree.</li><li><strong>Is the optimizer choosing the best join order?</strong> When you have a query that joins multiple tables, make sure the optimizer chooses the most selective join order. Joins that eliminate the largest number of rows should be done earlier in the plan so fewer rows move up the plan tree.</li></ul><p>See <a href="./query/topics/query-profiling.html">Query Profiling</a> for more information about reading query plans.</p><h3 id="tuning-statistics-collection" tabindex="-1"><a id="topic7"></a>Tuning Statistics Collection <a class="header-anchor" href="#tuning-statistics-collection" aria-label="Permalink to &quot;&lt;a id=&quot;topic7&quot;&gt;&lt;/a&gt;Tuning Statistics Collection&quot;">​</a></h3><p>The following configuration parameters control the amount of data sampled for statistics collection:</p><ul><li><code>default_statistics_target</code></li></ul><p>These parameters control statistics sampling at the system level. It is better to sample only increased statistics for columns used most frequently in query predicates. You can adjust statistics for a particular column using the command:</p><p><code>ALTER TABLE...SET STATISTICS</code></p><p>For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ALTER TABLE sales ALTER COLUMN region SET STATISTICS 50;</span></span></code></pre></div><p>This is equivalent to changing <code>default_statistics_target</code> for a particular column. Subsequent <code>ANALYZE</code> operations will then gather more statistics data for that column and produce better query plans as a result.</p><h2 id="optimizing-data-distribution" tabindex="-1"><a id="topic8"></a>Optimizing Data Distribution <a class="header-anchor" href="#optimizing-data-distribution" aria-label="Permalink to &quot;&lt;a id=&quot;topic8&quot;&gt;&lt;/a&gt;Optimizing Data Distribution&quot;">​</a></h2><p>When you create a table in WarehousePG, you must declare a distribution key that allows for even data distribution across all segments in the system. Because the segments work on a query in parallel, WarehousePG will always be as fast as the slowest segment. If the data is unbalanced, the segments that have more data will return their results slower and therefore slow down the entire system.</p><h2 id="optimizing-your-database-design" tabindex="-1"><a id="topic9"></a>Optimizing Your Database Design <a class="header-anchor" href="#optimizing-your-database-design" aria-label="Permalink to &quot;&lt;a id=&quot;topic9&quot;&gt;&lt;/a&gt;Optimizing Your Database Design&quot;">​</a></h2><p>Many performance issues can be improved by database design. Examine your database design and consider the following:</p><ul><li>Does the schema reflect the way the data is accessed?</li><li>Can larger tables be broken down into partitions?</li><li>Are you using the smallest data type possible to store column values?</li><li>Are columns used to join tables of the same datatype?</li><li>Are your indexes being used?</li></ul><h3 id="warehousepg-maximum-limits" tabindex="-1"><a id="topic10"></a>WarehousePG Maximum Limits <a class="header-anchor" href="#warehousepg-maximum-limits" aria-label="Permalink to &quot;&lt;a id=&quot;topic10&quot;&gt;&lt;/a&gt;WarehousePG Maximum Limits&quot;">​</a></h3><p>To help optimize database design, review the maximum limits that WarehousePG supports:</p><table tabindex="0"><thead><tr><th>Dimension</th><th>Limit</th></tr></thead><tbody><tr><td>Database Size</td><td>Unlimited</td></tr><tr><td>Table Size</td><td>Unlimited, 128 TB per partition per segment</td></tr><tr><td>Row Size</td><td>1.6 TB (1600 columns * 1 GB)</td></tr><tr><td>Field Size</td><td>1 GB</td></tr><tr><td>Rows per Table</td><td>281474976710656 (2^48)</td></tr><tr><td>Columns per Table/View</td><td>1600</td></tr><tr><td>Indexes per Table</td><td>Unlimited</td></tr><tr><td>Columns per Index</td><td>32</td></tr><tr><td>Table-level Constraints per Table</td><td>Unlimited</td></tr><tr><td>Table Name Length</td><td>63 Bytes (Limited by <em>name</em> data type)</td></tr></tbody></table><p>Dimensions listed as unlimited are not intrinsically limited by WarehousePG. However, they are limited in practice to available disk space and memory/swap space. Performance may degrade when these values are unusually large.</p><blockquote><p><strong>Note</strong> There is a maximum limit on the number of objects (tables, indexes, and views, but not rows) that may exist at one time. This limit is 4294967296 (2^32).</p></blockquote>',38)]))}const p=t(o,[["render",r]]);export{h as __pageData,p as default};
