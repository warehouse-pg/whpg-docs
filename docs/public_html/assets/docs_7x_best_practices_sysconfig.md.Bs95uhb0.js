import{_ as t,c as o,o as a,ag as s}from"./chunks/framework.Ds6Eueu6.js";const u=JSON.parse('{"title":"System Configuration","description":"","frontmatter":{},"headers":[],"relativePath":"docs/7x/best_practices/sysconfig.md","filePath":"docs/7x/best_practices/sysconfig.md"}'),i={name:"docs/7x/best_practices/sysconfig.md"};function n(r,e,l,c,m,p){return a(),o("div",null,e[0]||(e[0]=[s(`<h1 id="system-configuration" tabindex="-1">System Configuration <a class="header-anchor" href="#system-configuration" aria-label="Permalink to &quot;System Configuration&quot;">​</a></h1><hr><p>Requirements and best practices for system administrators who are configuring WarehousePG cluster hosts.</p><p>Configuration of the WarehousePG cluster is usually performed as root.</p><h2 id="configuring-the-timezone" tabindex="-1"><a id="topic_fvc_zh1_b2b"></a>Configuring the Timezone <a class="header-anchor" href="#configuring-the-timezone" aria-label="Permalink to &quot;&lt;a id=&quot;topic_fvc_zh1_b2b&quot;&gt;&lt;/a&gt;Configuring the Timezone&quot;">​</a></h2><p>WarehousePG selects a timezone to use from a set of internally stored PostgreSQL timezones. The available PostgreSQL timezones are taken from the Internet Assigned Numbers Authority (IANA) Time Zone Database, and WarehousePG updates its list of available timezones as necessary when the IANA database changes for PostgreSQL.</p><p>WarehousePG selects the timezone by matching a PostgreSQL timezone with the user specified time zone, or the host system time zone if no time zone is configured. For example, when selecting a default timezone, WarehousePG uses an algorithm to select a PostgreSQL timezone based on the host system timezone files. If the system timezone includes leap second information, WarehousePG cannot match the system timezone with a PostgreSQL timezone. In this case, WarehousePG calculates a &quot;best match&quot; with a PostgreSQL timezone based on information from the host system.</p><p>As a best practice, configure WarehousePG and the host systems to use a known, supported timezone. This sets the timezone for the WarehousePG coordinator and segment instances, and prevents WarehousePG from recalculating a &quot;best match&quot; timezone each time the cluster is restarted, using the current system timezone and WarehousePG timezone files (which may have been updated from the IANA database since the last restart). Use the <code>gpconfig</code> utility to show and set the WarehousePG timezone. For example, these commands show the WarehousePG timezone and set the timezone to <code>US/Pacific</code>.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># gpconfig -s TimeZone</span></span>
<span class="line"><span># gpconfig -c TimeZone -v &#39;US/Pacific&#39;</span></span></code></pre></div><p>You must restart WarehousePG after changing the timezone. The command <code>gpstop -ra</code> restarts WarehousePG. The catalog view <code>pg_timezone_names</code> provides WarehousePG timezone information.</p><h2 id="file-system" tabindex="-1"><a id="file_system"></a>File System <a class="header-anchor" href="#file-system" aria-label="Permalink to &quot;&lt;a id=&quot;file_system&quot;&gt;&lt;/a&gt;File System&quot;">​</a></h2><p>XFS is the file system used for WarehousePG data directories. Use the mount options described in <a href="./../install_guide/config_os.html">Configure Operating System</a>.</p><h2 id="port-configuration" tabindex="-1"><a id="port_config"></a>Port Configuration <a class="header-anchor" href="#port-configuration" aria-label="Permalink to &quot;&lt;a id=&quot;port_config&quot;&gt;&lt;/a&gt;Port Configuration&quot;">​</a></h2><p>See the <a href="./../install_guide/config_os.html#topic3">recommended OS parameter settings</a> in the <em>WarehousePG Installation Guide</em> for further details.</p><p>Set up <code>ip_local_port_range</code> so it does not conflict with the WarehousePG port ranges. For example, setting this range in <code>/etc/sysctl.conf</code>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>net.ipv4.ip_local_port_range = 10000  65535</span></span></code></pre></div><p>you could set the WarehousePG base port numbers to these values.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>PORT_BASE = 6000</span></span>
<span class="line"><span>MIRROR_PORT_BASE = 7000</span></span></code></pre></div><p>See the <a href="./../install_guide/config_os.html#topic3">Recommended OS Parameters Settings</a> in the <em>WarehousePG Installation Guide</em> for further details.</p><h2 id="i-o-configuration" tabindex="-1"><a id="io_config"></a>I/O Configuration <a class="header-anchor" href="#i-o-configuration" aria-label="Permalink to &quot;&lt;a id=&quot;io_config&quot;&gt;&lt;/a&gt;I/O Configuration&quot;">​</a></h2><p>Set the blockdev read-ahead size to 16384 on the devices that contain data directories. This command sets the read-ahead size for <code>/dev/sdb</code>.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># /sbin/blockdev --setra 16384 /dev/sdb</span></span></code></pre></div><p>This command returns the read-ahead size for <code>/dev/sdb</code>.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># /sbin/blockdev --getra /dev/sdb</span></span>
<span class="line"><span>16384</span></span></code></pre></div><p>See the <a href="./../install_guide/config_os.html#topic3">Recommended OS Parameters Settings</a> in the <em>WarehousePG Installation Guide</em> for further details.</p><p>The deadline IO scheduler should be set for all data directory devices.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span> # cat /sys/block/sdb/queue/scheduler</span></span>
<span class="line"><span> noop anticipatory [deadline] cfq</span></span></code></pre></div><p>The maximum number of OS files and processes should be increased in the <code>/etc/security/limits.conf</code> file.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>* soft  nofile 524288</span></span>
<span class="line"><span>* hard  nofile 524288</span></span>
<span class="line"><span>* soft  nproc 131072</span></span>
<span class="line"><span>* hard  nproc 131072</span></span></code></pre></div><h2 id="os-memory-configuration" tabindex="-1"><a id="os_mem_config"></a>OS Memory Configuration <a class="header-anchor" href="#os-memory-configuration" aria-label="Permalink to &quot;&lt;a id=&quot;os_mem_config&quot;&gt;&lt;/a&gt;OS Memory Configuration&quot;">​</a></h2><p>The Linux sysctl <code>vm.overcommit_memory</code> and <code>vm.overcommit_ratio</code> variables affect how the operating system manages memory allocation. See the <a href="./../install_guide/config_os.html#topic3__sysctl_file"><code>/etc/sysctl.conf</code></a> file parameters guidelines in the <em>WarehousePG Datatabase Installation Guide</em> for further details.</p><p><code>vm.overcommit_memory</code> determines the method the OS uses for determining how much memory can be allocated to processes. This should be always set to 2, which is the only safe setting for the database.</p><blockquote><p><strong>Note</strong> For information on configuration of overcommit memory, refer to:</p></blockquote><ul><li><a href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Memory_overcommitment&amp;sa=D&amp;ust=1499719618717000&amp;usg=AFQjCNErcHO7vErv4pn9fIhCxrR0XRiknA" target="_blank" rel="noreferrer">https://en.wikipedia.org/wiki/Memory_overcommitment</a></li><li><a href="https://www.google.com/url?q=https://www.kernel.org/doc/Documentation/vm/overcommit-accounting&amp;sa=D&amp;ust=1499719618717000&amp;usg=AFQjCNEmu5tZutAaN1KCSlIwz4hwqihkOQ" target="_blank" rel="noreferrer">https://www.kernel.org/doc/Documentation/vm/overcommit-accounting</a></li></ul><p><code>vm.overcommit_ratio</code> is the percent of RAM that is used for application processes. The default is 50 on Red Hat Enterprise Linux. See <a href="#segment_mem_config">Resource Queue Segment Memory Configuration</a> for a formula to calculate an optimal value.</p><p>Do not enable huge pages in the operating system.</p><p>See also <a href="./workloads.html">Memory and Resource Management with Resource Queues</a>.</p><h2 id="shared-memory-settings" tabindex="-1"><a id="shared_mem_config"></a>Shared Memory Settings <a class="header-anchor" href="#shared-memory-settings" aria-label="Permalink to &quot;&lt;a id=&quot;shared_mem_config&quot;&gt;&lt;/a&gt;Shared Memory Settings&quot;">​</a></h2><p>WarehousePG uses shared memory to communicate between <code>postgres</code> processes that are part of the same <code>postgres</code> instance. The following shared memory settings should be set in <code>sysctl</code> and are rarely modified. See the <a href="./../install_guide/config_os.html"><code>sysctl.conf</code></a> file parameters in the <em>WarehousePG Installation Guide</em> for further details.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>kernel.shmmax = 810810728448</span></span>
<span class="line"><span>kernel.shmmni = 4096</span></span>
<span class="line"><span>kernel.shmall = 197951838</span></span></code></pre></div><p>See <a href="./../install_guide/config_os.html#topic3/shared_memory_pages">Setting the WarehousePG Recommended OS Parameters</a> for more details.</p><h2 id="number-of-segments-per-host" tabindex="-1"><a id="host_segs"></a>Number of Segments per Host <a class="header-anchor" href="#number-of-segments-per-host" aria-label="Permalink to &quot;&lt;a id=&quot;host_segs&quot;&gt;&lt;/a&gt;Number of Segments per Host&quot;">​</a></h2><p>Determining the number of segments to run on each segment host has immense impact on overall system performance. The segments share the host&#39;s CPU cores, memory, and NICs with each other and with other processes running on the host. Over-estimating the number of segments a server can accommodate is a common cause of suboptimal performance.</p><p>The factors that must be considered when choosing how many segments to run per host include the following:</p><ul><li>Number of cores</li><li>Amount of physical RAM installed in the server</li><li>Number of NICs</li><li>Amount of storage attached to server</li><li>Mixture of primary and mirror segments</li><li>ETL processes that will run on the hosts</li><li>Non-WarehousePG processes running on the hosts</li></ul><h2 id="resource-queue-segment-memory-configuration" tabindex="-1"><a id="segment_mem_config"></a>Resource Queue Segment Memory Configuration <a class="header-anchor" href="#resource-queue-segment-memory-configuration" aria-label="Permalink to &quot;&lt;a id=&quot;segment_mem_config&quot;&gt;&lt;/a&gt;Resource Queue Segment Memory Configuration&quot;">​</a></h2><p>The <code>gp_vmem_protect_limit</code> server configuration parameter specifies the amount of memory that all active postgres processes for a single segment can consume at any given time. Queries that exceed this amount will fail. Use the following calculations to estimate a safe value for <code>gp_vmem_protect_limit</code>.</p><ol><li><p>Calculate <code>gp_vmem</code>, the host memory available to WarehousePG.</p><ul><li><p>If the total system memory is less than 256 GB, use this formula:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM)) / 1.7</span></span></code></pre></div></li><li><p>If the total system memory is equal to or greater than 256 GB, use this formula:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM)) / 1.17</span></span></code></pre></div></li></ul><p>where <code>SWAP</code> is the host&#39;s swap space in GB and <code>RAM</code> is the RAM installed on the host in GB.</p></li><li><p>Calculate <code>max_acting_primary_segments</code>. This is the maximum number of primary segments that can be running on a host when mirror segments are activated due to a segment or host failure on another host in the cluster. With mirrors arranged in a 4-host block with 8 primary segments per host, for example, a single segment host failure would activate two or three mirror segments on each remaining host in the failed host&#39;s block. The <code>max_acting_primary_segments</code> value for this configuration is 11 (8 primary segments plus 3 mirrors activated on failure).</p></li><li><p>Calculate <code>gp_vmem_protect_limit</code> by dividing the total WarehousePG memory by the maximum number of acting primaries:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>gp_vmem_protect_limit = gp_vmem / max_acting_primary_segments</span></span></code></pre></div><p>Convert to megabytes to find the value to set for the <code>gp_vmem_protect_limit</code> system configuration parameter.</p></li></ol><p>For scenarios where a large number of workfiles are generated, adjust the calculation for <code>gp_vmem</code> to account for the workfiles.</p><ul><li><p>If the total system memory is less than 256 GB:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM - (300KB * total_#_workfiles))) / 1.7</span></span></code></pre></div></li><li><p>If the total system memory is equal to or greater than 256 GB:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM - (300KB * total_#_workfiles))) / 1.17</span></span></code></pre></div></li></ul><p>For information about monitoring and managing workfile usage, see the <em>WarehousePG Administrator Guide</em>.</p><p>You can calculate the value of the <code>vm.overcommit_ratio</code> operating system parameter from the value of <code>gp_vmem</code>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vm.overcommit_ratio = (RAM - 0.026 * gp_vmem) / RAM</span></span></code></pre></div><p>See <a href="#os_mem_config">OS Memory Configuration</a> for more about about <code>vm.overcommit_ratio</code>.</p><p>See also <a href="./workloads.html">Memory and Resource Management with Resource Queues</a>.</p><h2 id="resource-queue-statement-memory-configuration" tabindex="-1"><a id="statement_mem_config"></a>Resource Queue Statement Memory Configuration <a class="header-anchor" href="#resource-queue-statement-memory-configuration" aria-label="Permalink to &quot;&lt;a id=&quot;statement_mem_config&quot;&gt;&lt;/a&gt;Resource Queue Statement Memory Configuration&quot;">​</a></h2><p>The <code>statement_mem</code> server configuration parameter is the amount of memory to be allocated to any single query in a segment database. If a statement requires additional memory it will spill to disk. Calculate the value for <code>statement_mem</code> with the following formula:</p><p><code>(gp_vmem_protect_limit * .9) / max_expected_concurrent_queries</code></p><p>For example, for 40 concurrent queries with <code>gp_vmem_protect_limit</code> set to 8GB (8192MB), the calculation for <code>statement_mem</code> would be:</p><p><code>(8192MB * .9) / 40 = 184MB</code></p><p>Each query would be allowed 184MB of memory before it must spill to disk.</p><p>To increase <code>statement_mem</code> safely you must either increase <code>gp_vmem_protect_limit</code> or reduce the number of concurrent queries. To increase <code>gp_vmem_protect_limit</code>, you must add physical RAM and/or swap space, or reduce the number of segments per host.</p><p>Note that adding segment hosts to the cluster cannot help out-of-memory errors unless you use the additional hosts to decrease the number of segments per host.</p><p>Spill files are created when there is not enough memory to fit all the mapper output, usually when 80% of the buffer space is occupied.</p><p>Also, see <a href="./workloads.html">Resource Management</a> for best practices for managing query memory using resource queues.</p><h2 id="resource-queue-spill-file-configuration" tabindex="-1"><a id="spill_files"></a>Resource Queue Spill File Configuration <a class="header-anchor" href="#resource-queue-spill-file-configuration" aria-label="Permalink to &quot;&lt;a id=&quot;spill_files&quot;&gt;&lt;/a&gt;Resource Queue Spill File Configuration&quot;">​</a></h2><p>WarehousePG creates <em>spill files</em> (also called <em>workfiles</em>) on disk if a query is allocated insufficient memory to run in memory. A single query can create no more than 100,000 spill files, by default, which is sufficient for the majority of queries.</p><p>You can control the maximum number of spill files created per query and per segment with the configuration parameter <code>gp_workfile_limit_files_per_query</code>. Set the parameter to 0 to allow queries to create an unlimited number of spill files. Limiting the number of spill files permitted prevents run-away queries from disrupting the system.</p><p>A query could generate a large number of spill files if not enough memory is allocated to it or if data skew is present in the queried data. If a query creates more than the specified number of spill files, WarehousePG returns this error:</p><p><code>ERROR: number of workfiles per query limit exceeded</code></p><p>Before raising the <code>gp_workfile_limit_files_per_query</code>, try reducing the number of spill files by changing the query, changing the data distribution, or changing the memory configuration.</p><p>The <code>gp_toolkit</code> schema includes views that allow you to see information about all the queries that are currently using spill files. This information can be used for troubleshooting and for tuning queries:</p><ul><li>The <code>gp_workfile_entries</code> view contains one row for each operator using disk space for workfiles on a segment at the current time. See <a href="./tuning_queries.html">How to Read Explain Plans</a> for information about operators.</li><li>The <code>gp_workfile_usage_per_query</code> view contains one row for each query using disk space for workfiles on a segment at the current time.</li><li>The <code>gp_workfile_usage_per_segment</code> view contains one row for each segment. Each row displays the total amount of disk space used for workfiles on the segment at the current time.</li></ul><p>See the <em>WarehousePG Reference Guide</em> for descriptions of the columns in these views.</p><p>The <code>gp_workfile_compression</code> configuration parameter specifies whether the spill files are compressed. It is <code>off</code> by default. Enabling compression can improve performance when spill files are used.</p><p><strong>Parent topic:</strong> <a href="./intro.html">WarehousePG Best Practices</a></p>`,76)]))}const d=t(i,[["render",n]]);export{u as __pageData,d as default};
