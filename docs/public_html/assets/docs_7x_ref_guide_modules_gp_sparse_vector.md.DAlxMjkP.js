import{_ as s,c as a,o as n,ag as t}from"./chunks/framework.Ds6Eueu6.js";const h=JSON.parse('{"title":"gp_sparse_vector","description":"","frontmatter":{},"headers":[],"relativePath":"docs/7x/ref_guide/modules/gp_sparse_vector.md","filePath":"docs/7x/ref_guide/modules/gp_sparse_vector.md"}'),o={name:"docs/7x/ref_guide/modules/gp_sparse_vector.md"};function p(i,e,c,d,l,r){return n(),a("div",null,e[0]||(e[0]=[t(`<h1 id="gp-sparse-vector" tabindex="-1">gp_sparse_vector <a class="header-anchor" href="#gp-sparse-vector" aria-label="Permalink to &quot;gp\\_sparse\\_vector&quot;">​</a></h1><p>The <code>gp_sparse_vector</code> module implements a WarehousePG data type and associated functions that use compressed storage of zeros to make vector computations on floating point numbers faster.</p><p>The <code>gp_sparse_vector</code> module is a WarehousePG extension.</p><h2 id="installing-and-registering-the-module" tabindex="-1"><a id="topic_reg"></a>Installing and Registering the Module <a class="header-anchor" href="#installing-and-registering-the-module" aria-label="Permalink to &quot;&lt;a id=&quot;topic_reg&quot;&gt;&lt;/a&gt;Installing and Registering the Module&quot;">​</a></h2><p>The <code>gp_sparse_vector</code> module is installed when you install WarehousePG. Before you can use any of the functions defined in the module, you must register the <code>gp_sparse_vector</code> extension in each database where you want to use the functions. Refer to <a href="./../../install_guide/install_extensions.html">Installing Extensions</a> for more information.</p><h2 id="using-the-gp-sparse-vector-module" tabindex="-1"><a id="topic_doc"></a>Using the gp_sparse_vector Module <a class="header-anchor" href="#using-the-gp-sparse-vector-module" aria-label="Permalink to &quot;&lt;a id=&quot;topic_doc&quot;&gt;&lt;/a&gt;Using the gp\\_sparse\\_vector Module&quot;">​</a></h2><p>When you use arrays of floating point numbers for various calculations, you will often have long runs of zeros. This is common in scientific, retail optimization, and text processing applications. Each floating point number takes 8 bytes of storage in memory and/or disk. Saving those zeros is often impractical. There are also many computations that benefit from skipping over the zeros.</p><p>For example, suppose the following array of <code>double</code>s is stored as a <code>float8[]</code> in WarehousePG:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>&#39;{0, 33, &lt;40,000 zeros&gt;, 12, 22 }&#39;::float8[]</span></span></code></pre></div><p>This type of array arises often in text processing, where a dictionary may have 40-100K terms and the number of words in a particular document is stored in a vector. This array would occupy slightly more than 320KB of memory/disk, most of it zeros. Any operation that you perform on this data works on 40,001 fields that are not important.</p><p>The WarehousePG built-in <code>array</code> datatype utilizes a bitmap for null values, but it is a poor choice for this use case because it is not optimized for <code>float8[]</code> or for long runs of zeros instead of nulls, and the bitmap is not run-length-encoding- (RLE) compressed. Even if each zero were stored as a <code>NULL</code> in the array, the bitmap for nulls would use 5KB to mark the nulls, which is not nearly as efficient as it could be.</p><p>The WarehousePG <code>gp_sparse_vector</code> module defines a data type and a simple RLE-based scheme that is biased toward being efficient for zero value bitmaps. This scheme uses only 6 bytes for bitmap storage.</p><blockquote><p><strong>Note</strong> The sparse vector data type defined by the <code>gp_sparse_vector</code> module is named <code>svec</code>. <code>svec</code> supports only <code>float8</code> vector values.</p></blockquote><p>You can construct an <code>svec</code> directly from a float array as follows:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT (&#39;{0, 13, 37, 53, 0, 71 }&#39;::float8[])::svec;</span></span></code></pre></div><p>The <code>gp_sparse_vector</code> module supports the vector operators <code>&lt;</code>, <code>&gt;</code>, <code>*</code>, <code>**</code>, <code>/</code>, <code>=</code>, <code>+</code>, <code>sum()</code>, <code>vec_count_nonzero()</code>, and so on. These operators take advantage of the efficient sparse storage format, making computations on <code>svec</code>s faster.</p><p>The plus (<code>+</code>) operator adds each of the terms of two vectors of the same dimension together. For example, if vector <code>a = {0,1,5}</code> and vector <code>b = {4,3,2}</code>, you would compute the vector addition as follows:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT (&#39;{0,1,5}&#39;::float8[]::svec + &#39;{4,3,2}&#39;::float8[]::svec)::float8[];</span></span>
<span class="line"><span> float8  </span></span>
<span class="line"><span>---------</span></span>
<span class="line"><span> {4,4,7}</span></span></code></pre></div><p>A vector dot product (<code>%*%</code>) between vectors <code>a</code> and <code>b</code> returns a scalar result of type <code>float8</code>. Compute the dot product (<code>(0*4+1*3+5*2)=13</code>) as follows:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT &#39;{0,1,5}&#39;::float8[]::svec %*% &#39;{4,3,2}&#39;::float8[]::svec;</span></span>
<span class="line"><span> ?column? </span></span>
<span class="line"><span>----------</span></span>
<span class="line"><span>       13</span></span></code></pre></div><p>Special vector aggregate functions are also useful. <code>sum()</code> is self explanatory. <code>vec_count_nonzero()</code> evaluates the count of non-zero terms found in a set of <code>svec</code> and returns an <code>svec</code> with the counts. For instance, for the set of vectors <code>{0,1,5},{10,0,3},{0,0,3},{0,1,0}</code>, the count of non-zero terms would be <code>{1,2,3}</code>. Use <code>vec_count_nonzero()</code> to compute the count of these vectors:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>CREATE TABLE listvecs( a svec );</span></span>
<span class="line"><span></span></span>
<span class="line"><span>INSERT INTO listvecs VALUES (&#39;{0,1,5}&#39;::float8[]),</span></span>
<span class="line"><span>    (&#39;{10,0,3}&#39;::float8[]),</span></span>
<span class="line"><span>    (&#39;{0,0,3}&#39;::float8[]),</span></span>
<span class="line"><span>    (&#39;{0,1,0}&#39;::float8[]);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>SELECT vec_count_nonzero( a )::float8[] FROM listvecs;</span></span>
<span class="line"><span> count_vec </span></span>
<span class="line"><span>-----------</span></span>
<span class="line"><span> {1,2,3}</span></span>
<span class="line"><span>(1 row)</span></span></code></pre></div><h2 id="additional-module-documentation" tabindex="-1"><a id="topic_info"></a>Additional Module Documentation <a class="header-anchor" href="#additional-module-documentation" aria-label="Permalink to &quot;&lt;a id=&quot;topic_info&quot;&gt;&lt;/a&gt;Additional Module Documentation&quot;">​</a></h2><p>Refer to the <code>gp_sparse_vector</code> READMEs in the <a href="https://github.com/greenplum-db/gpdb/tree/main/gpcontrib/gp_sparse_vector/README" target="_blank" rel="noreferrer">WarehousePG github repository</a> for additional information about this module.</p><p>Apache MADlib includes an extended implementation of sparse vectors. See the <a href="http://madlib.apache.org/docs/latest/group__grp__svec.html" target="_blank" rel="noreferrer">MADlib Documentation</a> for a description of this MADlib module.</p><h2 id="example" tabindex="-1"><a id="topic_examples"></a>Example <a class="header-anchor" href="#example" aria-label="Permalink to &quot;&lt;a id=&quot;topic_examples&quot;&gt;&lt;/a&gt;Example&quot;">​</a></h2><p>A text classification example that describes a dictionary and some documents follows. You will create WarehousePG tables representing a dictionary and some documents. You then perform document classification using vector arithmetic on word counts and proportions of dictionary words in each document.</p><p>Suppose that you have a dictionary composed of words in a text array. Create a table to store the dictionary data and insert some data (words) into the table. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>CREATE TABLE features (dictionary text[][]) DISTRIBUTED RANDOMLY;</span></span>
<span class="line"><span>INSERT INTO features </span></span>
<span class="line"><span>    VALUES (&#39;{am,before,being,bothered,corpus,document,i,in,is,me,never,now,&#39;</span></span>
<span class="line"><span>            &#39;one,really,second,the,third,this,until}&#39;);</span></span></code></pre></div><p>You have a set of documents, also defined as an array of words. Create a table to represent the documents and insert some data into the table:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>CREATE TABLE documents(docnum int, document text[]) DISTRIBUTED RANDOMLY;</span></span>
<span class="line"><span>INSERT INTO documents VALUES </span></span>
<span class="line"><span>    (1,&#39;{this,is,one,document,in,the,corpus}&#39;),</span></span>
<span class="line"><span>    (2,&#39;{i,am,the,second,document,in,the,corpus}&#39;),</span></span>
<span class="line"><span>    (3,&#39;{being,third,never,really,bothered,me,until,now}&#39;),</span></span>
<span class="line"><span>    (4,&#39;{the,document,before,me,is,the,third,document}&#39;);</span></span></code></pre></div><p>Using the dictionary and document tables, find the dictionary words that are present in each document. To do this, you first prepare a <em>Sparse Feature Vector</em>, or SFV, for each document. An SFV is a vector of dimension <em>N</em>, where <em>N</em> is the number of dictionary words, and each SFV contains a count of each dictionary word in the document.</p><p>You can use the <code>gp_extract_feature_histogram()</code> function to create an SFV from a document. <code>gp_extract_feature_histogram()</code> outputs an <code>svec</code> for each document that contains the count of each of the dictionary words in the ordinal positions of the dictionary.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT gp_extract_feature_histogram(</span></span>
<span class="line"><span>    (SELECT dictionary FROM features LIMIT 1), document)::float8[], document</span></span>
<span class="line"><span>        FROM documents ORDER BY docnum;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>     gp_extract_feature_histogram        |                     document                         </span></span>
<span class="line"><span>-----------------------------------------+--------------------------------------------------</span></span>
<span class="line"><span> {0,0,0,0,1,1,0,1,1,0,0,0,1,0,0,1,0,1,0} | {this,is,one,document,in,the,corpus}</span></span>
<span class="line"><span> {1,0,0,0,1,1,1,1,0,0,0,0,0,0,1,2,0,0,0} | {i,am,the,second,document,in,the,corpus}</span></span>
<span class="line"><span> {0,0,1,1,0,0,0,0,0,1,1,1,0,1,0,0,1,0,1} | {being,third,never,really,bothered,me,until,now}</span></span>
<span class="line"><span> {0,1,0,0,0,2,0,0,1,1,0,0,0,0,0,2,1,0,0} | {the,document,before,me,is,the,third,document}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>SELECT * FROM features;</span></span>
<span class="line"><span>                                               dictionary</span></span>
<span class="line"><span>--------------------------------------------------------------------------------------------------------</span></span>
<span class="line"><span> {am,before,being,bothered,corpus,document,i,in,is,me,never,now,one,really,second,the,third,this,until}</span></span></code></pre></div><p>The SFV of the second document, &quot;i am the second document in the corpus&quot;, is <code>{1,3*0,1,1,1,1,6*0,1,2}</code>. The word &quot;am&quot; is the first ordinate in the dictionary, and there is <code>1</code> instance of it in the SFV. The word &quot;before&quot; has no instances in the document, so its value is <code>0</code>; and so on.</p><p><code>gp_extract_feature_histogram()</code> is very speed optimized - it is a single routine version of a hash join that processes large numbers of documents into their SFVs in parallel at the highest possible speeds.</p><p>For the next part of the processing, generate a sparse vector of the dictionary dimension (19). The vectors that you generate for each document are referred to as the <em>corpus</em>.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>CREATE table corpus (docnum int, feature_vector svec) DISTRIBUTED RANDOMLY;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>INSERT INTO corpus</span></span>
<span class="line"><span>    (SELECT docnum, </span></span>
<span class="line"><span>        gp_extract_feature_histogram(</span></span>
<span class="line"><span>            (select dictionary FROM features LIMIT 1), document) from documents);</span></span></code></pre></div><p>Count the number of times each feature occurs at least once in all documents:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT (vec_count_nonzero(feature_vector))::float8[] AS count_in_document FROM corpus;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>            count_in_document</span></span>
<span class="line"><span>-----------------------------------------</span></span>
<span class="line"><span> {1,1,1,1,2,3,1,2,2,2,1,1,1,1,1,3,2,1,1}</span></span></code></pre></div><p>Count all occurrences of each term in all documents:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT (sum(feature_vector))::float8[] AS sum_in_document FROM corpus;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>             sum_in_document</span></span>
<span class="line"><span>-----------------------------------------</span></span>
<span class="line"><span> {1,1,1,1,2,4,1,2,2,2,1,1,1,1,1,5,2,1,1}</span></span></code></pre></div><p>The remainder of the classification process is vector math. The count is turned into a weight that reflects <em>Term Frequency / Inverse Document Frequency</em> (tf/idf). The calculation for a given term in a given document is:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>#_times_term_appears_in_this_doc * log( #_docs / #_docs_the_term_appears_in )</span></span></code></pre></div><p><code>#_docs</code> is the total number of documents (4 in this case). Note that there is one divisor for each dictionary word and its value is the number of times that word appears in the document.</p><p>For example, the term &quot;document&quot; in document 1 would have a weight of <code>1 * log( 4/3 )</code>. In document 4, the term would have a weight of <code>2 * log( 4/3 )</code>. Terms that appear in every document would have weight <code>0</code>.</p><p>This single vector for the whole corpus is then scalar product multiplied by each document SFV to produce the tf/idf.</p><p>Calculate the tf/idf:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT docnum, (feature_vector*logidf)::float8[] AS tf_idf </span></span>
<span class="line"><span>    FROM (SELECT log(count(feature_vector)/vec_count_nonzero(feature_vector)) AS logidf FROM corpus)</span></span>
<span class="line"><span>    AS foo, corpus ORDER BY docnum;</span></span>
<span class="line"><span> docnum |                                                                          tf_idf                                                                          </span></span>
<span class="line"><span>--------+----------------------------------------------------------------------------------------------------------------------------------------------------------</span></span>
<span class="line"><span>      1 | {0,0,0,0,0.693147180559945,0.287682072451781,0,0.693147180559945,0.693147180559945,0,0,0,1.38629436111989,0,0,0.287682072451781,0,1.38629436111989,0}</span></span>
<span class="line"><span>      2 | {1.38629436111989,0,0,0,0.693147180559945,0.287682072451781,1.38629436111989,0.693147180559945,0,0,0,0,0,0,1.38629436111989,0.575364144903562,0,0,0}</span></span>
<span class="line"><span>      3 | {0,0,1.38629436111989,1.38629436111989,0,0,0,0,0,0.693147180559945,1.38629436111989,1.38629436111989,0,1.38629436111989,0,0,0.693147180559945,0,1.38629436111989</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>      4 | {0,1.38629436111989,0,0,0,0.575364144903562,0,0,0.693147180559945,0.693147180559945,0,0,0,0,0,0.575364144903562,0.693147180559945,0,0}</span></span></code></pre></div><p>You can determine the <em>angular distance</em> between one document and the rest of the documents using the ACOS of the dot product of the document vectors:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>CREATE TABLE weights AS </span></span>
<span class="line"><span>    (SELECT docnum, (feature_vector*logidf) tf_idf </span></span>
<span class="line"><span>        FROM (SELECT log(count(feature_vector)/vec_count_nonzero(feature_vector))</span></span>
<span class="line"><span>       AS logidf FROM corpus) foo, corpus ORDER BY docnum)</span></span>
<span class="line"><span>    DISTRIBUTED RANDOMLY;</span></span></code></pre></div><p>Calculate the angular distance between the first document and every other document:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT docnum, trunc((180.*(ACOS(dmin(1.,(tf_idf%*%testdoc)/(l2norm(tf_idf)*l2norm(testdoc))))/(4.*ATAN(1.))))::numeric,2)</span></span>
<span class="line"><span>     AS angular_distance FROM weights,</span></span>
<span class="line"><span>     (SELECT tf_idf testdoc FROM weights WHERE docnum = 1 LIMIT 1) foo</span></span>
<span class="line"><span>ORDER BY 1;</span></span>
<span class="line"><span></span></span>
<span class="line"><span> docnum | angular_distance </span></span>
<span class="line"><span>--------+------------------</span></span>
<span class="line"><span>      1 |             0.00</span></span>
<span class="line"><span>      2 |            78.82</span></span>
<span class="line"><span>      3 |            90.00</span></span>
<span class="line"><span>      4 |            80.02</span></span></code></pre></div><p>You can see that the angular distance between document 1 and itself is 0 degrees, and between document 1 and 3 is 90 degrees because they share no features at all.</p>`,54)]))}const m=s(o,[["render",p]]);export{h as __pageData,m as default};
