import{_ as s,c as a,o as t,ag as o}from"./chunks/framework.Ds6Eueu6.js";const u=JSON.parse('{"title":"Configure Operating System","description":"","frontmatter":{},"headers":[],"relativePath":"docs/7x/install_guide/config_os.md","filePath":"docs/7x/install_guide/config_os.md"}'),n={name:"docs/7x/install_guide/config_os.md"};function i(r,e,p,d,c,l){return t(),a("div",null,e[0]||(e[0]=[o(`<h1 id="configure-operating-system" tabindex="-1">Configure Operating System <a class="header-anchor" href="#configure-operating-system" aria-label="Permalink to &quot;Configure Operating System&quot;">​</a></h1><hr><p>Describes how to prepare your operating system environment for WarehousePG software installation.</p><p>Perform the following tasks in order:</p><ol><li>Make sure your host systems meet the requirements described in <a href="./platform-requirements.html#on-prem">On-Premise Hardware Requirements</a>.</li><li><a href="#topic_sqj_lt1_nfb">Deactivate or configure SELinux.</a></li><li><a href="#topic_et2_y22_4nb">Deactivate or configure firewall software.</a></li><li><a href="#topic3">Set the required operating system parameters.</a></li><li><a href="#topic_qst_s5t_wy">Synchronize system clocks.</a></li><li><a href="#topic23">Create the gpadmin account.</a></li></ol><p>Unless noted, these tasks should be performed for <em>all</em> hosts in your WarehousePG cluster (coordinator, standby coordinator, and segment hosts).</p><p>The WarehousePG host naming convention for the coordinator host is <code>cdw</code> and for the standby coordinator host is <code>scdw</code>.</p><p>The segment host naming convention is sdwN where sdw is a prefix and N is an integer. For example, segment host names would be <code>sdw1</code>, <code>sdw2</code> and so on. NIC bonding is recommended for hosts with multiple interfaces, but when the interfaces are not bonded, the convention is to append a dash (<code>-</code>) and number to the host name. For example, <code>sdw1-1</code> and <code>sdw1-2</code> are the two interface names for host <code>sdw1</code>.</p><blockquote><p><strong>Important</strong> When data loss is not acceptable for a WarehousePG cluster, WarehousePG coordinator and segment mirroring is recommended. If mirroring is not enabled then WarehousePG stores only one copy of the data, so the underlying storage media provides the only guarantee for data availability and correctness in the event of a hardware failure.</p></blockquote><p>The WarehousePG on vSphere virtualized environment ensures the enforcement of anti-affinity rules required for WarehousePG mirroring solutions and fully supports mirrorless deployments. Other virtualized or containerized deployment environments are generally not supported for production use unless both WarehousePG coordinator and segment mirroring are enabled.</p><blockquote><p><strong>Note</strong> For information about upgrading WarehousePG from a previous version, see the <em>WarehousePG Release Notes</em> for the release that you are installing.</p></blockquote><blockquote><p><strong>Note</strong> Automating the configuration steps described in this topic and <a href="./install_whpg.html">Installing WarehousePG</a> with a system provisioning tool, such as Ansible, Chef, or Puppet, can save time and ensure a reliable and repeatable WarehousePG installation.</p></blockquote><p><strong>Parent topic:</strong> <a href="./install_guide/">Installing and Upgrading WarehousePG</a></p><h2 id="deactivate-or-configure-selinux" tabindex="-1"><a id="topic_sqj_lt1_nfb"></a>Deactivate or Configure SELinux <a class="header-anchor" href="#deactivate-or-configure-selinux" aria-label="Permalink to &quot;&lt;a id=&quot;topic_sqj_lt1_nfb&quot;&gt;&lt;/a&gt;Deactivate or Configure SELinux&quot;">​</a></h2><p>For all WarehousePG host systems running RHEL/Oracle/Rocky Linux, SELinux must either be <code>Disabled</code> or configured to allow unconfined access to WarehousePG processes, directories, and the gpadmin user.</p><p>If you choose to deactivate SELinux:</p><ol><li><p>As the root user, check the status of SELinux:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># sestatus</span></span>
<span class="line"><span>SELinuxstatus: disabled</span></span></code></pre></div></li><li><p>If SELinux is not deactivated, deactivate it by editing the <code>/etc/selinux/config</code> file. As root, change the value of the <code>SELINUX</code> parameter in the <code>config</code> file as follows:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELINUX=disabled</span></span></code></pre></div></li><li><p>If the System Security Services Daemon (SSSD) is installed on your systems, edit the SSSD configuration file and set the <code>selinux_provider</code> parameter to <code>none</code> to prevent SELinux-related SSH authentication denials that could occur even with SELinux deactivated. As root, edit <code>/etc/sssd/sssd.conf</code> and add this parameter:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>selinux_provider=none</span></span></code></pre></div></li><li><p>Reboot the system to apply any changes that you made and verify that SELinux is deactivated.</p></li></ol><p>If you choose to enable SELinux in <code>Enforcing</code> mode, then WarehousePG processes and users can operate successfully in the default <code>Unconfined</code> context. If you require increased SELinux confinement for WarehousePG processes and users, you must test your configuration to ensure that there are no functionality or performance impacts to WarehousePG. See the <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/selinux_users_and_administrators_guide/index" target="_blank" rel="noreferrer">SELinux User&#39;s and Administrator&#39;s Guide</a> for detailed information about configuring SELinux and SELinux users.</p><h2 id="deactivate-or-configure-firewall-software" tabindex="-1"><a id="topic_et2_y22_4nb"></a>Deactivate or Configure Firewall Software <a class="header-anchor" href="#deactivate-or-configure-firewall-software" aria-label="Permalink to &quot;&lt;a id=&quot;topic_et2_y22_4nb&quot;&gt;&lt;/a&gt;Deactivate or Configure Firewall Software&quot;">​</a></h2><p>You should also deactivate firewall software such as <code>firewalld</code> (on systems such as RHEL). If firewall software is not deactivated, you must instead configure your software to allow required communication between WarehousePG hosts.</p><ol><li><p>Check the status of <code>firewalld</code> with the command:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># systemctl status firewalld</span></span></code></pre></div><p>If <code>firewalld</code> is deactivated, the command output is:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>* firewalld.service - firewalld - dynamic firewall daemon</span></span>
<span class="line"><span>   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span></span>
<span class="line"><span>   Active: inactive (dead)</span></span></code></pre></div></li><li><p>If necessary, run these commands as root to deactivate <code>firewalld</code>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># systemctl stop firewalld.service</span></span>
<span class="line"><span># systemctl disable firewalld.service</span></span></code></pre></div></li></ol><p>See the documentation for the firewall or your operating system for additional information.</p><h2 id="recommended-os-parameters-settings" tabindex="-1"><a id="topic3"></a>Recommended OS Parameters Settings <a class="header-anchor" href="#recommended-os-parameters-settings" aria-label="Permalink to &quot;&lt;a id=&quot;topic3&quot;&gt;&lt;/a&gt;Recommended OS Parameters Settings&quot;">​</a></h2><p>WarehousePG requires that certain Linux operating system (OS) parameters be set on all hosts in your WarehousePG cluster (coordinators and segments).</p><p>In general, the following categories of system parameters need to be altered:</p><ul><li><strong>Shared Memory</strong> - A WarehousePG instance will not work unless the shared memory segment for your kernel is properly sized. Most default OS installations have the shared memory values set too low for WarehousePG. On Linux systems, you must also deactivate the OOM (out of memory) killer. For information about WarehousePG shared memory requirements, see the WarehousePG server configuration parameter <a href="./../ref_guide/config_params/guc-list.html">shared_buffers</a> in the <em>WarehousePG Reference Guide</em>.</li><li><strong>Network</strong> - On high-volume WarehousePG clusters, certain network-related tuning parameters must be set to optimize network connections made by the WarehousePG interconnect.</li><li><strong>User Limits</strong> - User limits control the resources available to processes started by a user&#39;s shell. WarehousePG requires a higher limit on the allowed number of file descriptors that a single process can have open. The default settings may cause some WarehousePG queries to fail because they will run out of file descriptors needed to process the query.</li></ul><p>More specifically, you need to edit the following Linux configuration settings:</p><ul><li><a href="#linux_hosts_file">The hosts File</a></li><li><a href="#sysctl_file">The sysctl.conf File</a></li><li><a href="#system_resources">System Resources Limits</a></li><li><a href="#core_dump">Core Dump</a></li><li><a href="#xfs_mount">XFS Mount Options</a></li><li><a href="#disk_io_settings">Disk I/O Settings</a><ul><li>Read ahead values</li><li>Disk I/O scheduler disk access</li></ul></li><li><a href="#networking">Networking</a></li><li><a href="#huge_pages">Transparent Huge Pages (THP)</a></li><li><a href="#ipc_object_removal">IPC Object Removal</a></li><li><a href="#ssh_max_connections">SSH Connection Threshold</a></li></ul><h3 id="the-hosts-file" tabindex="-1"><a id="linux_hosts_file"></a>The hosts File <a class="header-anchor" href="#the-hosts-file" aria-label="Permalink to &quot;&lt;a id=&quot;linux_hosts_file&quot;&gt;&lt;/a&gt;The hosts File&quot;">​</a></h3><p>Edit the <code>/etc/hosts</code> file and make sure that it includes the host names and all interface address names for every machine participating in your WarehousePG cluster.</p><h3 id="the-sysctl-conf-file" tabindex="-1"><a id="sysctl_file"></a>The sysctl.conf File <a class="header-anchor" href="#the-sysctl-conf-file" aria-label="Permalink to &quot;&lt;a id=&quot;sysctl_file&quot;&gt;&lt;/a&gt;The sysctl.conf File&quot;">​</a></h3><p>The <code>sysctl.conf</code> parameters listed in this topic are for performance, optimization, and consistency in a wide variety of environments. Change these settings according to your specific situation and setup.</p><p>Set the parameters in the <code>/etc/sysctl.conf</code> file and reload with <code>sysctl -p</code>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span></span></span>
<span class="line"><span># kernel.shmall = _PHYS_PAGES / 2 # See Shared Memory Pages</span></span>
<span class="line"><span>kernel.shmall = 197951838</span></span>
<span class="line"><span># kernel.shmmax = kernel.shmall * PAGE_SIZE </span></span>
<span class="line"><span>kernel.shmmax = 810810728448</span></span>
<span class="line"><span>kernel.shmmni = 4096</span></span>
<span class="line"><span>vm.overcommit_memory = 2 # See Segment Host Memory</span></span>
<span class="line"><span>vm.overcommit_ratio = 95 # See Segment Host Memory</span></span>
<span class="line"><span></span></span>
<span class="line"><span>net.ipv4.ip_local_port_range = 10000 65535 # See Port Settings</span></span>
<span class="line"><span>kernel.sem = 250 2048000 200 8192</span></span>
<span class="line"><span>kernel.sysrq = 1</span></span>
<span class="line"><span>kernel.core_uses_pid = 1</span></span>
<span class="line"><span>kernel.msgmnb = 65536</span></span>
<span class="line"><span>kernel.msgmax = 65536</span></span>
<span class="line"><span>kernel.msgmni = 2048</span></span>
<span class="line"><span>net.ipv4.tcp_syncookies = 1</span></span>
<span class="line"><span>net.ipv4.conf.default.accept_source_route = 0</span></span>
<span class="line"><span>net.ipv4.tcp_max_syn_backlog = 4096</span></span>
<span class="line"><span>net.ipv4.conf.all.arp_filter = 1</span></span>
<span class="line"><span>net.ipv4.ipfrag_high_thresh = 41943040</span></span>
<span class="line"><span>net.ipv4.ipfrag_low_thresh = 31457280</span></span>
<span class="line"><span>net.ipv4.ipfrag_time = 60</span></span>
<span class="line"><span>net.core.netdev_max_backlog = 10000</span></span>
<span class="line"><span>net.core.rmem_max = 2097152</span></span>
<span class="line"><span>net.core.wmem_max = 2097152</span></span>
<span class="line"><span>vm.swappiness = 10</span></span>
<span class="line"><span>vm.zone_reclaim_mode = 0</span></span>
<span class="line"><span>vm.dirty_expire_centisecs = 500</span></span>
<span class="line"><span>vm.dirty_writeback_centisecs = 100</span></span>
<span class="line"><span>vm.dirty_background_ratio = 0 # See System Memory</span></span>
<span class="line"><span>vm.dirty_ratio = 0</span></span>
<span class="line"><span>vm.dirty_background_bytes = 1610612736</span></span>
<span class="line"><span>vm.dirty_bytes = 4294967296</span></span></code></pre></div><p><strong>Shared Memory Pages</strong></p><p>WarehousePG uses shared memory to communicate between <code>postgres</code> processes that are part of the same <code>postgres</code> instance. <code>kernel.shmall</code> sets the total amount of shared memory, in pages, that can be used system wide. <code>kernel.shmmax</code> sets the maximum size of a single shared memory segment in bytes.</p><p>Set <code>kernel.shmall</code> and <code>kernel.shmmax</code> values based on your system&#39;s physical memory and page size. In general, the value for both parameters should be one half of the system physical memory.</p><p>Use the operating system variables <code>_PHYS_PAGES</code> and <code>PAGE_SIZE</code> to set the parameters.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>kernel.shmall = ( _PHYS_PAGES / 2)</span></span>
<span class="line"><span>kernel.shmmax = ( _PHYS_PAGES / 2) * PAGE_SIZE</span></span></code></pre></div><p>To calculate the values for <code>kernel.shmall</code> and <code>kernel.shmmax</code>, run the following commands using the <code>getconf</code> command, which returns the value of an operating system variable.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>$ echo $(expr $(getconf _PHYS_PAGES) / 2) </span></span>
<span class="line"><span>$ echo $(expr $(getconf _PHYS_PAGES) / 2 \\* $(getconf PAGE_SIZE))</span></span></code></pre></div><p>As best practice, we recommend you set the following values in the <code>/etc/sysctl.conf</code> file using calculated values. For example, a host system has 1583 GB of memory installed and returns these values: _PHYS_PAGES = 395903676 and PAGE_SIZE = 4096. These would be the <code>kernel.shmall</code> and <code>kernel.shmmax</code> values:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>kernel.shmall = 197951838</span></span>
<span class="line"><span>kernel.shmmax = 810810728448</span></span></code></pre></div><p>If the WarehousePG coordinator has a different shared memory configuration than the segment hosts, the _PHYS_PAGES and PAGE_SIZE values might differ, and the <code>kernel.shmall</code> and <code>kernel.shmmax</code> values on the coordinator host will differ from those on the segment hosts.</p><p><strong>Segment Host Memory</strong></p><p>The <code>vm.overcommit_memory</code> Linux kernel parameter is used by the OS to determine how much memory can be allocated to processes. For WarehousePG, this parameter should always be set to 2.</p><p><code>vm.overcommit_ratio</code> is the percent of RAM that is used for application processes and the remainder is reserved for the operating system. The default is 50 on Red Hat Enterprise Linux.</p><p>For <code>vm.overcommit_ratio</code> tuning and calculation recommendations with resource group-based resource management or resource queue-based resource management, refer to <a href="./../admin_guide/wlmgmt_intro.html">Options for Configuring Segment Host Memory</a> in the <em>WarehousePG Administrator Guide</em>.</p><p><strong>Port Settings</strong></p><p>To avoid port conflicts between WarehousePG and other applications during WarehousePG initialization, make a note of the port range specified by the operating system parameter <code>net.ipv4.ip_local_port_range</code>. When initializing WarehousePG using the <code>gpinitsystem</code> cluster configuration file, do not specify WarehousePG ports in that range. For example, if <code>net.ipv4.ip_local_port_range = 10000 65535</code>, set the WarehousePG base port numbers to these values.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>PORT_BASE = 6000</span></span>
<span class="line"><span>MIRROR_PORT_BASE = 7000</span></span></code></pre></div><p>For information about the <code>gpinitsystem</code> cluster configuration file, see <a href="./../install_guide/init_whpg.html">Initializing WarehousePG</a>.</p><p>For Azure deployments with WarehousePG avoid using port 65330; add the following line to sysctl.conf:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>net.ipv4.ip_local_reserved_ports=65330</span></span></code></pre></div><p>For additional requirements and recommendations for cloud deployments, see <a href="./platform-requirements.html#public-cloud">Public Cloud Requirements</a>.</p><p><strong>IP Fragmentation Settings</strong></p><p>When the WarehousePG interconnect uses UDP (the default), the network interface card controls IP packet fragmentation and reassemblies.</p><p>If the UDP message size is larger than the size of the maximum transmission unit (MTU) of a network, the IP layer fragments the message. (Refer to <a href="#networking">Networking</a> later in this topic for more information about MTU sizes for WarehousePG.) The receiver must store the fragments in a buffer before it can reorganize and reassemble the message.</p><p>The following <code>sysctl.conf</code> operating system parameters control the reassembly process:</p><table tabindex="0"><thead><tr><th>OS Parameter</th><th>Description</th></tr></thead><tbody><tr><td>net.ipv4.ipfrag_high_thresh</td><td>The maximum amount of memory used to reassemble IP fragments before the kernel starts to remove fragments to free up resources. The default value is 4194304 bytes (4MB).</td></tr><tr><td>net.ipv4.ipfrag_low_thresh</td><td>The minimum amount of memory used to reassemble IP fragments. The default value is 3145728 bytes (3MB). (Deprecated after kernel version 4.17.)</td></tr><tr><td>net.ipv4.ipfrag_time</td><td>The maximum amount of time (in seconds) to keep an IP fragment in memory. The default value is 30.</td></tr></tbody></table><p>The recommended settings for these parameters for WarehousePG follow:</p><div class="language-pre vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">pre</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>net.ipv4.ipfrag_high_thresh = 41943040</span></span>
<span class="line"><span>net.ipv4.ipfrag_low_thresh = 31457280</span></span>
<span class="line"><span>net.ipv4.ipfrag_time = 60</span></span></code></pre></div><p><strong>System Memory</strong></p><p>For host systems with more than 64GB of memory, these settings are recommended:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vm.dirty_background_ratio = 0</span></span>
<span class="line"><span>vm.dirty_ratio = 0</span></span>
<span class="line"><span>vm.dirty_background_bytes = 1610612736 # 1.5GB</span></span>
<span class="line"><span>vm.dirty_bytes = 4294967296 # 4GB</span></span></code></pre></div><p>For host systems with 64GB of memory or less, remove <code>vm.dirty_background_bytes</code> and <code>vm.dirty_bytes</code> and set the two <code>ratio</code> parameters to these values:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vm.dirty_background_ratio = 3</span></span>
<span class="line"><span>vm.dirty_ratio = 10</span></span></code></pre></div><p>Increase <code>vm.min_free_kbytes</code> to ensure <code>PF_MEMALLOC</code> requests from network and storage drivers are easily satisfied. This is especially critical on systems with large amounts of system memory. The default value is often far too low on these systems. Use this awk command to set <code>vm.min_free_kbytes</code> to a recommended 3% of system physical memory:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>awk &#39;BEGIN {OFMT = &quot;%.0f&quot;;} /MemTotal/ {print &quot;vm.min_free_kbytes =&quot;, $2 * .03;}&#39;</span></span>
<span class="line"><span>               /proc/meminfo &gt;&gt; /etc/sysctl.conf</span></span></code></pre></div><p>Do not set <code>vm.min_free_kbytes</code> to higher than 5% of system memory as doing so might cause out of memory conditions.</p><h3 id="system-resources-limits" tabindex="-1"><a id="system_resources"></a>System Resources Limits <a class="header-anchor" href="#system-resources-limits" aria-label="Permalink to &quot;&lt;a id=&quot;system_resources&quot;&gt;&lt;/a&gt;System Resources Limits&quot;">​</a></h3><p>Set the following parameters in the <code>/etc/security/limits.conf</code> file:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>* soft nofile 524288</span></span>
<span class="line"><span>* hard nofile 524288</span></span>
<span class="line"><span>* soft nproc 131072</span></span>
<span class="line"><span>* hard nproc 131072</span></span></code></pre></div><p>For Red Hat Enterprise Linux (RHEL) systems, parameter values in the <code>/etc/security/limits.d/20-nproc.conf</code> file override the values in the <code>limits.conf</code> file. Ensure that any parameters in the override file are set to the required value. The Linux module <code>pam_limits</code> sets user limits by reading the values from the <code>limits.conf</code> file and then from the override file. For information about PAM and user limits, see the documentation on PAM and <code>pam_limits</code>.</p><p>Run the <code>ulimit -u</code> command on each segment host to display the maximum number of processes that are available to each user. Validate that the return value is 131072.</p><h3 id="core-dump" tabindex="-1"><a id="core_dump"></a>Core Dump <a class="header-anchor" href="#core-dump" aria-label="Permalink to &quot;&lt;a id=&quot;core_dump&quot;&gt;&lt;/a&gt;Core Dump&quot;">​</a></h3><p>Enable core file generation to a known location by adding the following line to <code>/etc/sysctl.conf</code>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>kernel.core_pattern=/var/core/core.%h.%t</span></span></code></pre></div><p>Add the following line to <code>/etc/security/limits.conf</code>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>* soft  core unlimited</span></span></code></pre></div><p>To apply the changes to the live kernel, run the following command:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># sysctl -p</span></span></code></pre></div><h3 id="xfs-mount-options" tabindex="-1"><a id="xfs_mount"></a>XFS Mount Options <a class="header-anchor" href="#xfs-mount-options" aria-label="Permalink to &quot;&lt;a id=&quot;xfs_mount&quot;&gt;&lt;/a&gt;XFS Mount Options&quot;">​</a></h3><p>XFS is the preferred data storage file system on Linux platforms. Use the <code>mount</code> command with the following recommended XFS mount options for RHEL systems:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>rw,nodev,noatime,inode64</span></span></code></pre></div><p>See the <code>mount</code> manual page (<code>man mount</code> opens the man page) for more information about using this command.</p><p>The XFS options can also be set in the <code>/etc/fstab</code> file. This example entry from an <code>fstab</code> file specifies the XFS options.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>/dev/data /data xfs nodev,noatime,inode64 0 0</span></span></code></pre></div><blockquote><p><strong>Note</strong> You must have root permission to edit the <code>/etc/fstab</code> file.</p></blockquote><h3 id="disk-i-o-settings" tabindex="-1"><a id="disk_io_settings"></a>Disk I/O Settings <a class="header-anchor" href="#disk-i-o-settings" aria-label="Permalink to &quot;&lt;a id=&quot;disk_io_settings&quot;&gt;&lt;/a&gt;Disk I/O Settings&quot;">​</a></h3><ul><li><p>Read-ahead value</p><p>Each disk device file should have a read-ahead (<code>blockdev</code>) value of 16384. To verify the read-ahead value of a disk device:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># sudo /sbin/blockdev --getra &lt;devname&gt;</span></span></code></pre></div><p>For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># sudo /sbin/blockdev --getra /dev/sdb</span></span></code></pre></div><p>To set blockdev (read-ahead) on a device:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># sudo /sbin/blockdev --setra &lt;bytes&gt; &lt;devname&gt;</span></span></code></pre></div><p>For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># sudo /sbin/blockdev --setra 16384 /dev/sdb</span></span></code></pre></div><p>See the manual page (man) for the <code>blockdev</code> command for more information about using that command (<code>man blockdev</code> opens the man page).</p><blockquote><p><strong>Note</strong> The <code>blockdev --setra</code> command is not persistent. You must ensure the read-ahead value is set whenever the system restarts. How to set the value will vary based on your system.</p></blockquote><p>One method to set the <code>blockdev</code> value at system startup is by adding the <code>/sbin/blockdev --setra</code> command in the <code>rc.local</code> file. For example, add this line to the <code>rc.local</code> file to set the read-ahead value for the disk <code>sdb</code>.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>/sbin/blockdev --setra 16384 /dev/sdb</span></span></code></pre></div><p>On systems that use systemd, you must also set the execute permissions on the <code>rc.local</code> file to enable it to run at startup. For example, on a RHEL system, this command sets execute permissions on the file.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># chmod +x /etc/rc.d/rc.local</span></span></code></pre></div><p>Restart the system to have the setting take effect.</p></li><li><p>Disk I/O scheduler</p><p>The Linux disk scheduler orders the I/O requests submitted to a storage device, controlling the way the kernel commits reads and writes to disk.</p><p>A typical Linux disk I/O scheduler supports multiple access policies. The optimal policy selection depends on the underlying storage infrastructure. The recommended scheduler policy settings for WarehousePG clusters for specific OSs and storage device types follow:</p><table><thead><tr><th>Storage Device Type</th><th>OS</th><th>Recommended Scheduler Policy</th></tr></thead><tbody><tr><td>Non-Volatile Memory Express (NVMe)</td><td>RHEL 7RHEL 8Ubuntu</td><td><code>none</code></td></tr><tr><td rowspan="2">Solid-State Drives (SSD)</td><td>RHEL 7</td><td><code>noop</code></td></tr><tr><td>RHEL 8Ubuntu</td><td><code>none</code></td></tr><tr><td rowspan="2">Other</td><td>RHEL 7</td><td><code>deadline</code></td></tr><tr><td>RHEL 8Ubuntu</td><td><code>mq-deadline</code></td></tr></tbody></table><p>To specify a scheduler until the next system reboot, run the following:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># echo schedulername &gt; /sys/block/&lt;devname&gt;/queue/scheduler</span></span></code></pre></div><p>For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># echo deadline &gt; /sys/block/sbd/queue/scheduler</span></span></code></pre></div><blockquote><p><strong>Note</strong> Using the <code>echo</code> command to set the disk I/O scheduler policy is not persistent; you must ensure that you run the command whenever the system reboots. How to run the command will vary based on your system.</p></blockquote><p>To specify the I/O scheduler at boot time on systems that use <code>grub2</code>, use the system utility <code>grubby</code>. This command adds the parameter when run as <code>root</code>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># grubby --update-kernel=ALL --args=&quot;elevator=deadline&quot;</span></span></code></pre></div><p>After adding the parameter, reboot the system.</p><p>This <code>grubby</code> command displays kernel parameter settings:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># grubby --info=ALL</span></span></code></pre></div><p>Refer to your operating system documentation for more information about the <code>grubby</code> utility. If you used the <code>grubby</code> command to configure the disk scheduler on a RHEL system and it does not update the kernels, see the <a href="#grubby_note">Note</a> at the end of the section.</p><p>For additional information about configuring the disk scheduler, refer to the RedHat Enterprise Linux documentation for <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/setting-the-disk-scheduler_monitoring-and-managing-system-status-and-performance" target="_blank" rel="noreferrer">RHEL 8</a>.</p></li></ul><h3 id="networking" tabindex="-1"><a id="networking"></a>Networking <a class="header-anchor" href="#networking" aria-label="Permalink to &quot;&lt;a id=&quot;networking&quot;&gt;&lt;/a&gt;Networking&quot;">​</a></h3><p>The maximum transmission unit (MTU) of a network specifies the size (in bytes) of the largest data packet/frame accepted by a network-connected device. A jumbo frame is a frame that contains more than the standard MTU of 1500 bytes.</p><p>WarehousePG utilizes 3 distinct MTU settings:</p><ul><li>The WarehousePG <a href="./../ref_guide/config_params/guc-list.html#gp_max_packet_size">gp_max_packet_size</a> server configuration parameter. The default max packet size is 8192. This default assumes a jumbo frame MTU.</li><li>The operating system MTU setting.</li><li>The rack switch MTU setting.</li></ul><p>These settings are connected, in that they should always be either the same, or close to the same, value, or otherwise in the order of WarehousePG &lt; OS &lt; switch for MTU size.</p><p>9000 is a common supported setting for switches, and is the recommended OS and rack switch MTU setting for your WarehousePG hosts.</p><h3 id="transparent-huge-pages-thp" tabindex="-1"><a id="huge_pages"></a>Transparent Huge Pages (THP) <a class="header-anchor" href="#transparent-huge-pages-thp" aria-label="Permalink to &quot;&lt;a id=&quot;huge_pages&quot;&gt;&lt;/a&gt;Transparent Huge Pages \\(THP\\)&quot;">​</a></h3><p>Deactivate Transparent Huge Pages (THP) as it degrades WarehousePG performance. RHEL 6.0 or higher enables THP by default. One way to deactivate THP on RHEL 6.x is by adding the parameter <code>transparent_hugepage=never</code> to the kernel command in the file <code>/boot/grub/grub.conf</code>, the GRUB boot loader configuration file. This is an example kernel command from a <code>grub.conf</code> file. The command is on multiple lines for readability:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>kernel /vmlinuz-2.6.18-274.3.1.el5 ro root=LABEL=/</span></span>
<span class="line"><span>           elevator=deadline crashkernel=128M@16M  quiet console=tty1</span></span>
<span class="line"><span>           console=ttyS1,115200 panic=30 transparent_hugepage=never </span></span>
<span class="line"><span>           initrd /initrd-2.6.18-274.3.1.el5.img</span></span></code></pre></div><p>On systems that use <code>grub2</code>, use the system utility <code>grubby</code>. This command adds the parameter when run as root.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># grubby --update-kernel=ALL --args=&quot;transparent_hugepage=never&quot;</span></span></code></pre></div><p>After adding the parameter, reboot the system.</p><p>For Ubuntu systems, install the <code>hugepages</code> package and run this command as root:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># hugeadm --thp-never</span></span></code></pre></div><p>This cat command checks the state of THP. The output indicates that THP is deactivated.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>$ cat /sys/kernel/mm/*transparent_hugepage/enabled</span></span>
<span class="line"><span>always [never]</span></span></code></pre></div><p>For more information about Transparent Huge Pages or the <code>grubby</code> utility, see your operating system documentation. If the <code>grubby</code> command does not update the kernels, see the <a href="#grubby_note">Note</a> at the end of the section.</p><h3 id="ipc-object-removal" tabindex="-1"><a id="ipc_object_removal"></a>IPC Object Removal <a class="header-anchor" href="#ipc-object-removal" aria-label="Permalink to &quot;&lt;a id=&quot;ipc_object_removal&quot;&gt;&lt;/a&gt;IPC Object Removal&quot;">​</a></h3><p>Deactivate IPC object removal. The default <code>systemd</code> setting <code>RemoveIPC=yes</code> removes IPC connections when non-system user accounts log out. This causes the WarehousePG utility <code>gpinitsystem</code> to fail with semaphore errors. Perform one of the following to avoid this issue.</p><ul><li><p>When you add the <code>gpadmin</code> operating system user account to the coordinator node in <a href="#topic23">Creating the WarehousePG Administrative User</a>, create the user as a system account.</p></li><li><p>Deactivate <code>RemoveIPC</code>. Set this parameter in <code>/etc/systemd/logind.conf</code> on the WarehousePG host systems.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>RemoveIPC=no</span></span></code></pre></div><p>The setting takes effect after restarting the <code>systemd-login</code> service or rebooting the system. To restart the service, run this command as the root user.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>service systemd-logind restart</span></span></code></pre></div></li></ul><h3 id="ssh-connection-threshold" tabindex="-1"><a id="ssh_max_connections"></a>SSH Connection Threshold <a class="header-anchor" href="#ssh-connection-threshold" aria-label="Permalink to &quot;&lt;a id=&quot;ssh_max_connections&quot;&gt;&lt;/a&gt;SSH Connection Threshold&quot;">​</a></h3><p>Certain WarehousePG management utilities including <code>gpexpand</code>, <code>gpinitsystem</code>, and <code>gpaddmirrors</code>, use secure shell (SSH) connections between systems to perform their tasks. In large WarehousePG deployments, cloud deployments, or deployments with a large number of segments per host, these utilities may exceed the host&#39;s maximum threshold for unauthenticated connections. When this occurs, you receive errors such as: <code>ssh_exchange_identification: Connection closed by remote host</code>.</p><p>To increase this connection threshold for your WarehousePG cluster, update the SSH <code>MaxStartups</code> and <code>MaxSessions</code> configuration parameters in one of the <code>/etc/ssh/sshd_config</code> or <code>/etc/sshd_config</code> SSH daemon configuration files.</p><blockquote><p><strong>Note</strong> You must have root permission to edit these two files.</p></blockquote><p>If you specify <code>MaxStartups</code> and <code>MaxSessions</code> using a single integer value, you identify the maximum number of concurrent unauthenticated connections (<code>MaxStartups</code>) and maximum number of open shell, login, or subsystem sessions permitted per network connection (<code>MaxSessions</code>). For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>MaxStartups 200</span></span>
<span class="line"><span>MaxSessions 200</span></span></code></pre></div><p>If you specify <code>MaxStartups</code> using the &quot;start:rate:full&quot; syntax, you enable random early connection drop by the SSH daemon. start identifies the maximum number of unauthenticated SSH connection attempts allowed. Once start number of unauthenticated connection attempts is reached, the SSH daemon refuses rate percent of subsequent connection attempts. full identifies the maximum number of unauthenticated connection attempts after which all attempts are refused. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Max Startups 10:30:200</span></span>
<span class="line"><span>MaxSessions 200</span></span></code></pre></div><p>Restart the SSH daemon after you update <code>MaxStartups</code> and <code>MaxSessions</code>. For example, on a CentOS 6 system, run the following command as the <code>root</code> user:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># service sshd restart</span></span></code></pre></div><p>For detailed information about SSH configuration options, refer to the SSH documentation for your Linux distribution.</p><p><a id="grubby_note"></a></p><blockquote><p><strong>Note</strong> If the <code>grubby</code> command does not update the kernels of a RHEL 7.x or CentOS 7.x system, you can manually update all kernels on the system. For example, to add the parameter <code>transparent_hugepage=never</code> to all kernels on a system.</p></blockquote><ol><li><p>Add the parameter to the <code>GRUB_CMDLINE_LINUX</code> line in the file parameter in <code>/etc/default/grub</code>.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>GRUB_TIMEOUT=5</span></span>
<span class="line"><span>GRUB_DISTRIBUTOR=&quot;$(sed &#39;s, release .*$,,g&#39; /etc/system-release)&quot;</span></span>
<span class="line"><span>GRUB_DEFAULT=saved</span></span>
<span class="line"><span>GRUB_DISABLE_SUBMENU=true</span></span>
<span class="line"><span>GRUB_TERMINAL_OUTPUT=&quot;console&quot;</span></span>
<span class="line"><span>GRUB_CMDLINE_LINUX=&quot;crashkernel=auto rd.lvm.lv=cl/root rd.lvm.lv=cl/swap rhgb quiet transparent_hugepage=never&quot;</span></span>
<span class="line"><span>              GRUB_DISABLE_RECOVERY=&quot;true&quot;</span></span></code></pre></div><blockquote><p><strong>Note</strong> You must have root permission to edit the <code>/etc/default/grub</code> file.</p></blockquote></li><li><p>As root, run the <code>grub2-mkconfig</code> command to update the kernels.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># grub2-mkconfig -o /boot/grub2/grub.cfg</span></span></code></pre></div></li><li><p>Reboot the system.</p></li></ol><h2 id="synchronizing-system-clocks" tabindex="-1"><a id="topic_qst_s5t_wy"></a>Synchronizing System Clocks <a class="header-anchor" href="#synchronizing-system-clocks" aria-label="Permalink to &quot;&lt;a id=&quot;topic_qst_s5t_wy&quot;&gt;&lt;/a&gt;Synchronizing System Clocks&quot;">​</a></h2><p>You must use NTP (Network Time Protocol) to synchronize the system clocks on all hosts that comprise your WarehousePG cluster. Accurate time keeping is essential to ensure reliable operations on the database and data integrity.</p><p>There are many different architectures you may choose from to implement NTP. We recommend you use one of the following:</p><ul><li>Configure coordinator as the NTP primary source and the other hosts in the cluster connect to it.</li><li>Configure an external NTP primary source and all hosts in the cluster connect to it.</li></ul><p>Depending on your operating system version, the NTP protocol may be implemented by the <code>ntpd</code> daemon, the <code>chronyd</code> daemon, or other. Refer to your preferred NTP protocol documentation for more details.</p><h3 id="option-1-configure-system-clocks-with-the-coordinator-as-the-primary-source" tabindex="-1"><a id="ji162603"></a>Option 1: Configure System Clocks with the Coordinator as the Primary Source <a class="header-anchor" href="#option-1-configure-system-clocks-with-the-coordinator-as-the-primary-source" aria-label="Permalink to &quot;&lt;a id=&quot;ji162603&quot;&gt;&lt;/a&gt;Option 1: Configure System Clocks with the Coordinator as the Primary Source&quot;">​</a></h3><ol><li><p>On the coordinator host, log in as root and edit your NTP daemon configuration file. Set the <code>server</code> parameter to point to your data center&#39;s NTP time server. For example (if <code>10.6.220.20</code> was the IP address of your data center&#39;s NTP server):</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>server 10.6.220.20</span></span></code></pre></div></li><li><p>On each segment host, log in as root and edit your NTP daemon configuration file. Set the first <code>server</code> parameter to point to the coordinator host, and the second server parameter to point to the standby coordinator host. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>server cdw prefer</span></span>
<span class="line"><span>server scdw</span></span></code></pre></div></li><li><p>On the standby coordinator host, log in as root and edit your NTP daemon configuration file. Set the first <code>server</code> parameter to point to the primary coordinator host, and the second server parameter to point to your data center&#39;s NTP time server. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>server cdw prefer</span></span>
<span class="line"><span>server 10.6.220.20</span></span></code></pre></div></li><li><p>Synchronize the system clocks on all WarehousePG hosts as root.</p><p>If you are using the <code>ntpd</code> daemon:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>systemctl restart ntp</span></span></code></pre></div><p>If you are using the <code>chronyd</code> daemon:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>systemctl restart chronyd</span></span></code></pre></div></li></ol><h3 id="option-2-configure-system-clocks-with-an-external-primary-source" tabindex="-1"><a id="ji162603"></a>Option 2: Configure System Clocks with an External Primary Source <a class="header-anchor" href="#option-2-configure-system-clocks-with-an-external-primary-source" aria-label="Permalink to &quot;&lt;a id=&quot;ji162603&quot;&gt;&lt;/a&gt;Option 2: Configure System Clocks with an External Primary Source&quot;">​</a></h3><ol><li><p>On each host, including coordinator, standby coordinator, and segments, log in as root and edit your NTP daemon configuration file. Set the first <code>server</code> parameter to point to your data center&#39;s NTP time server. For example (if <code>10.6.220.20</code> was the IP address of your data center&#39;s NTP server):</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>server 10.6.220.20</span></span></code></pre></div></li><li><p>On the coordinator host, use your NTP daemon to synchronize the system clocks on all WarehousePG hosts. For example, using <a href="./../utility_guide/ref/gpssh.html">gpssh</a>:</p><p>If you are using the <code>ntpd</code> daemon:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>gpssh -f hostfile_gpssh_allhosts -v -e &#39;ntpd&#39;</span></span></code></pre></div><p>If you are using the <code>chronyd</code> daemon:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>gpssh -f hostfile_gpssh_allhosts -v -e &#39;chronyd&#39;</span></span></code></pre></div></li></ol><h2 id="creating-the-warehousepg-administrative-user" tabindex="-1"><a id="topic23"></a>Creating the WarehousePG Administrative User <a class="header-anchor" href="#creating-the-warehousepg-administrative-user" aria-label="Permalink to &quot;&lt;a id=&quot;topic23&quot;&gt;&lt;/a&gt;Creating the WarehousePG Administrative User&quot;">​</a></h2><p>Create a dedicated operating system user account on each node to run and administer WarehousePG. This user account is named <code>gpadmin</code> by convention.</p><blockquote><p><strong>Important</strong> You cannot run the WarehousePG server as <code>root</code>.</p></blockquote><p>The <code>gpadmin</code> user must have permission to access the services and directories required to install and run WarehousePG.</p><p>The <code>gpadmin</code> user on each WarehousePG host must have an SSH key pair installed and be able to SSH from any host in the cluster to any other host in the cluster without entering a password or passphrase (called &quot;passwordless SSH&quot;). If you enable passwordless SSH from the coordinator host to every other host in the cluster (&quot;1-<em>n</em> passwordless SSH&quot;), you can use the WarehousePG <code>gpssh-exkeys</code> command-line utility later to enable passwordless SSH from every host to every other host (&quot;<em>n</em>-<em>n</em> passwordless SSH&quot;).</p><p>You can optionally give the <code>gpadmin</code> user sudo privilege, so that you can easily administer all hosts in the WarehousePG cluster as <code>gpadmin</code> using the <code>sudo</code>, <code>ssh/rsync</code>, and <code>gpssh/gpsync</code> commands.</p><p>The following steps show how to set up the <code>gpadmin</code> user on a host, set a password, create an SSH key pair, and (optionally) enable sudo capability. These steps must be performed as root on every WarehousePG cluster host. (For a large WarehousePG cluster you will want to automate these steps using your system provisioning tools.)</p><ol><li><p>Create the <code>gpadmin</code> group and user.</p><blockquote><p><strong>Note</strong> If you are installing WarehousePG on RHEL 7.2 or CentOS 7.2 and want to deactivate IPC object removal by creating the <code>gpadmin</code> user as a system account, provide both the <code>-r</code> option (create the user as a system account) and the <code>-m</code> option (create a home directory) to the <code>useradd</code> command. On Ubuntu systems, you must use the <code>-m</code> option with the <code>useradd</code> command to create a home directory for a user.</p></blockquote><p>This example creates the <code>gpadmin</code> group, creates the <code>gpadmin</code> user as a system account with a home directory and as a member of the <code>gpadmin</code> group, and creates a password for the user.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># groupadd gpadmin</span></span>
<span class="line"><span># useradd gpadmin -r -m -g gpadmin</span></span>
<span class="line"><span># passwd gpadmin</span></span>
<span class="line"><span>New password: &lt;changeme&gt;</span></span>
<span class="line"><span>Retype new password: &lt;changeme&gt;</span></span></code></pre></div><blockquote><p><strong>Note</strong> You must have root permission to create the <code>gpadmin</code> group and user.</p></blockquote><blockquote><p><strong>Note</strong> Make sure the <code>gpadmin</code> user has the same user id (uid) and group id (gid) numbers on each host to prevent problems with scripts or services that use them for identity or permissions. For example, backing up WarehousePGs to some networked filesy stems or storage appliances could fail if the <code>gpadmin</code> user has different uid or gid numbers on different segment hosts. When you create the <code>gpadmin</code> group and user, you can use the <code>groupadd -g</code> option to specify a gid number and the <code>useradd -u</code> option to specify the uid number. Use the command <code>id gpadmin</code> to see the uid and gid for the <code>gpadmin</code> user on the current host.</p></blockquote></li><li><p>Switch to the <code>gpadmin</code> user and generate an SSH key pair for the <code>gpadmin</code> user.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>$ su gpadmin</span></span>
<span class="line"><span>$ ssh-keygen -t rsa -b 4096</span></span>
<span class="line"><span>Generating public/private rsa key pair.</span></span>
<span class="line"><span>Enter file in which to save the key (/home/gpadmin/.ssh/id_rsa):</span></span>
<span class="line"><span>Created directory &#39;/home/gpadmin/.ssh&#39;.</span></span>
<span class="line"><span>Enter passphrase (empty for no passphrase):</span></span>
<span class="line"><span>Enter same passphrase again:</span></span></code></pre></div><p>At the passphrase prompts, press Enter so that SSH connections will not require entry of a passphrase.</p></li><li><p>Grant sudo access to the <code>gpadmin</code> user.</p><p>On Red Hat or CentOS, run <code>visudo</code> and uncomment the <code>%wheel</code> group entry.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%wheel        ALL=(ALL)       NOPASSWD: ALL</span></span></code></pre></div><p>Make sure you uncomment the line that has the <code>NOPASSWD</code> keyword.</p><p>Add the <code>gpadmin</code> user to the <code>wheel</code> group with this command.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># usermod -aG wheel gpadmin</span></span></code></pre></div></li></ol><h2 id="next-steps" tabindex="-1"><a id="topic_acx_5xb_vhb"></a>Next Steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;&lt;a id=&quot;topic_acx_5xb_vhb&quot;&gt;&lt;/a&gt;Next Steps&quot;">​</a></h2><ul><li><a href="./install_whpg.html">Installing WarehousePG Software</a></li><li><a href="./validate.html">Validating the WHPG Cluster</a></li><li><a href="./init_whpg.html">Initializing WarehousePG</a></li></ul>`,144)]))}const m=s(n,[["render",i]]);export{u as __pageData,m as default};
