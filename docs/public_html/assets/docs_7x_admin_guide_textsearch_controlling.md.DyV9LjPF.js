import{_ as s,c as a,o as t,ag as n}from"./chunks/framework.Ds6Eueu6.js";const u=JSON.parse('{"title":"Controlling Text Search","description":"","frontmatter":{},"headers":[],"relativePath":"docs/7x/admin_guide/textsearch/controlling.md","filePath":"docs/7x/admin_guide/textsearch/controlling.md"}'),o={name:"docs/7x/admin_guide/textsearch/controlling.md"};function i(r,e,p,c,l,d){return t(),a("div",null,e[0]||(e[0]=[n(`<h1 id="controlling-text-search" tabindex="-1">Controlling Text Search <a class="header-anchor" href="#controlling-text-search" aria-label="Permalink to &quot;Controlling Text Search&quot;">​</a></h1><hr><p>This topic shows how to create search and query vectors, how to rank search results, and how to highlight search terms in the results of text search queries.</p><p>To implement full text searching there must be a function to create a <code>tsvector</code> from a document and a <code>tsquery</code> from a user query. Also, we need to return results in a useful order, so we need a function that compares documents with respect to their relevance to the query. It&#39;s also important to be able to display the results nicely. WarehousePG provides support for all of these functions.</p><p>This topic contains the following subtopics:</p><ul><li><a href="#parsing-documents">Parsing Documents</a></li><li><a href="#parsing-queries">Parsing Queries</a></li><li><a href="#ranking">Ranking Search Results</a></li><li><a href="#highlighting">Highlighting Results</a></li></ul><h2 id="parsing-documents" tabindex="-1"><a id="parsing-documents"></a>Parsing Documents <a class="header-anchor" href="#parsing-documents" aria-label="Permalink to &quot;&lt;a id=&quot;parsing-documents&quot;&gt;&lt;/a&gt;Parsing Documents&quot;">​</a></h2><p>WarehousePG provides the function <code>to_tsvector</code> for converting a document to the <code>tsvector</code> data type.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>to_tsvector([&lt;config&gt; regconfig, ] &lt;document&gt; text) returns tsvector</span></span></code></pre></div><p><code>to_tsvector</code> parses a textual document into tokens, reduces the tokens to lexemes, and returns a <code>tsvector</code> which lists the lexemes together with their positions in the document. The document is processed according to the specified or default text search configuration. Here is a simple example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT to_tsvector(&#39;english&#39;, &#39;a fat  cat sat on a mat - it ate a fat rats&#39;);</span></span>
<span class="line"><span>                  to_tsvector</span></span>
<span class="line"><span>-----------------------------------------------------</span></span>
<span class="line"><span> &#39;ate&#39;:9 &#39;cat&#39;:3 &#39;fat&#39;:2,11 &#39;mat&#39;:7 &#39;rat&#39;:12 &#39;sat&#39;:4</span></span></code></pre></div><p>In the example above we see that the resulting tsvector does not contain the words <code>a</code>, <code>on</code>, or <code>it</code>, the word <code>rats</code> became <code>rat</code>, and the punctuation sign <code>-</code> was ignored.</p><p>The <code>to_tsvector</code> function internally calls a parser which breaks the document text into tokens and assigns a type to each token. For each token, a list of dictionaries (<a href="./dictionaries.html">Text Search Dictionaries</a>) is consulted, where the list can vary depending on the token type. The first dictionary that <em>recognizes</em> the token emits one or more normalized <em>lexemes</em> to represent the token. For example, <code>rats</code> became <code>rat</code> because one of the dictionaries recognized that the word <code>rats</code> is a plural form of <code>rat</code>. Some words are recognized as <em>stop words</em>, which causes them to be ignored since they occur too frequently to be useful in searching. In our example these are <code>a</code>, <code>on</code>, and <code>it</code>. If no dictionary in the list recognizes the token then it is also ignored. In this example that happened to the punctuation sign <code>-</code> because there are in fact no dictionaries assigned for its token type (<code>Space symbols</code>), meaning space tokens will never be indexed. The choices of parser, dictionaries and which types of tokens to index are determined by the selected text search configuration (<a href="./configuration.html">Text Search Configuration Example</a>). It is possible to have many different configurations in the same database, and predefined configurations are available for various languages. In our example we used the default configuration <code>english</code> for the English language.</p><p>The function <code>setweight</code> can be used to label the entries of a <code>tsvector</code> with a given <em>weight</em>, where a weight is one of the letters <code>A</code>, <code>B</code>, <code>C</code>, or <code>D</code>. This is typically used to mark entries coming from different parts of a document, such as <code>title</code> versus <code>body</code>. Later, this information can be used for ranking of search results.</p><p>Because <code>to_tsvector(NULL)</code> will return <code>NULL</code>, it is recommended to use <code>coalesce</code> whenever a field might be null. Here is the recommended method for creating a tsvector from a structured document:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>UPDATE tt SET ti = setweight(to_tsvector(coalesce(title,&#39;&#39;)), &#39;A&#39;) </span></span>
<span class="line"><span>  || setweight(to_tsvector(coalesce(keyword,&#39;&#39;)), &#39;B&#39;) </span></span>
<span class="line"><span>  || setweight(to_tsvector(coalesce(abstract,&#39;&#39;)), &#39;C&#39;) </span></span>
<span class="line"><span>  || setweight(to_tsvector(coalesce(body,&#39;&#39;)), &#39;D&#39;);</span></span></code></pre></div><p>Here we have used <code>setweight</code> to label the source of each lexeme in the finished <code>tsvector</code>, and then merged the labeled <code>tsvector</code> values using the <code>tsvector</code> concatenation operator <code>||</code>. (<a href="./features.html">Additional Text Search Features</a> gives details about these operations.)</p><h2 id="parsing-queries" tabindex="-1"><a id="parsing-queries"></a>Parsing Queries <a class="header-anchor" href="#parsing-queries" aria-label="Permalink to &quot;&lt;a id=&quot;parsing-queries&quot;&gt;&lt;/a&gt;Parsing Queries&quot;">​</a></h2><p>WarehousePG provides the functions <code>to_tsquery</code>, <code>plainto_tsquery</code>, <code>phraseto_tsquery</code>, and <code>websearch_to_tsquery</code> for converting a query to the <code>tsquery</code> data type. <code>to_tsquery</code> offers access to more features than <code>plainto_tsquery</code>, but is less forgiving about its input. <code>websearch_to_tsquery</code> is a simplified version of <code>to_tsquery</code> with an alternative syntax, similar to the one used by web search engines.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>to_tsquery([&lt;config&gt; regconfig, ] &lt;querytext&gt; text) returns tsquery</span></span></code></pre></div><p><code>to_tsquery</code> creates a <code>tsquery</code> value from <em>querytext</em>, which must consist of single tokens separated by the Boolean operators <code>&amp;</code> (AND), <code>|</code> (OR), <code>!</code> (NOT), and &lt;-&gt; (FOLLOWED BY), possibly grouped using parentheses. In other words, the input to <code>to_tsquery</code> must already follow the general rules for tsquery input, as described in <a href="./../../ref_guide/datatype-textsearch.html">Text Search Data Types</a>. The difference is that while basic <code>tsquery</code> input takes the tokens at face value, <code>to_tsquery</code> normalizes each token to a lexeme using the specified or default configuration, and discards any tokens that are stop words according to the configuration. For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT to_tsquery(&#39;english&#39;, &#39;The &amp; Fat &amp; Rats&#39;);</span></span>
<span class="line"><span>  to_tsquery   </span></span>
<span class="line"><span>---------------</span></span>
<span class="line"><span> &#39;fat&#39; &amp; &#39;rat&#39;</span></span></code></pre></div><p>As in basic <code>tsquery</code> input, weight(s) can be attached to each lexeme to restrict it to match only <code>tsvector</code> lexemes of those weight(s). For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT to_tsquery(&#39;english&#39;, &#39;Fat | Rats:AB&#39;);</span></span>
<span class="line"><span>    to_tsquery    </span></span>
<span class="line"><span>------------------</span></span>
<span class="line"><span> &#39;fat&#39; | &#39;rat&#39;:AB</span></span></code></pre></div><p>Also, <code>*</code> can be attached to a lexeme to specify prefix matching:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT to_tsquery(&#39;supern:*A &amp; star:A*B&#39;);</span></span>
<span class="line"><span>        to_tsquery        </span></span>
<span class="line"><span>--------------------------</span></span>
<span class="line"><span> &#39;supern&#39;:*A &amp; &#39;star&#39;:*AB</span></span></code></pre></div><p>Such a lexeme will match any word in a <code>tsvector</code> that begins with the given string.</p><p><code>to_tsquery</code> can also accept single-quoted phrases. This is primarily useful when the configuration includes a thesaurus dictionary that may trigger on such phrases. In the example below, a thesaurus contains the rule <code>supernovae stars : sn</code>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT to_tsquery(&#39;&#39;&#39;supernovae stars&#39;&#39; &amp; !crab&#39;);</span></span>
<span class="line"><span>  to_tsquery</span></span>
<span class="line"><span>---------------</span></span>
<span class="line"><span> &#39;sn&#39; &amp; !&#39;crab&#39;</span></span></code></pre></div><p>Without quotes, <code>to_tsquery</code> will generate a syntax error for tokens that are not separated by an AND, OR, or FOLLOWED BY operator.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>plainto_tsquery([ &lt;config&gt; regconfig, ] &lt;querytext&gt; ext) returns tsquery</span></span></code></pre></div><p><code>plainto_tsquery</code> transforms unformatted text <code>*querytext*</code> to <code>tsquery</code>. The text is parsed and normalized much as for <code>to_tsvector</code>, then the <code>&amp;</code> (AND) Boolean operator is inserted between surviving words.</p><p>Example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT plainto_tsquery(&#39;english&#39;, &#39;The Fat Rats&#39;);</span></span>
<span class="line"><span> plainto_tsquery </span></span>
<span class="line"><span>-----------------</span></span>
<span class="line"><span> &#39;fat&#39; &amp; &#39;rat&#39;</span></span></code></pre></div><p>Note that <code>plainto_tsquery</code> cannot recognize Boolean operators, weight labels, or prefix-match labels in its input:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT plainto_tsquery(&#39;english&#39;, &#39;The Fat &amp; Rats:C&#39;);</span></span>
<span class="line"><span>   plainto_tsquery   </span></span>
<span class="line"><span>---------------------</span></span>
<span class="line"><span> &#39;fat&#39; &amp; &#39;rat&#39; &amp; &#39;c&#39;</span></span></code></pre></div><p>Here, all the input punctuation was discarded as being space symbols.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>phraseto_tsquery([ &lt;config&gt; regconfig, ] &lt;querytext&gt; text) returns tsquery</span></span></code></pre></div><p><code>phraseto_tsquery</code> behaves much like <code>plainto_tsquery</code>, except that it inserts the <code>&lt;-&gt;</code> (FOLLOWED BY) operator between surviving words instead of the <code>&amp;</code> (AND) operator. Also, stop words are not simply discarded, but are accounted for by inserting <code>&lt;N&gt;</code> operators rather than <code>&lt;-&gt;</code> operators. This function is useful when searching for exact lexeme sequences, since the FOLLOWED BY operators check lexeme order not just the presence of all the lexemes.</p><p>Example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT phraseto_tsquery(&#39;english&#39;, &#39;The Fat Rats&#39;);</span></span>
<span class="line"><span> phraseto_tsquery</span></span>
<span class="line"><span>------------------</span></span>
<span class="line"><span> &#39;fat&#39; &lt;-&gt; &#39;rat&#39;</span></span></code></pre></div><p>Like <code>plainto_tsquery</code>, the <code>phraseto_tsquery</code> function will not recognize <code>tsquery</code> operators, weight labels, or prefix-match labels in its input:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT phraseto_tsquery(&#39;english&#39;, &#39;The Fat &amp; Rats:C&#39;);</span></span>
<span class="line"><span>      phraseto_tsquery</span></span>
<span class="line"><span>-----------------------------</span></span>
<span class="line"><span> &#39;fat&#39; &lt;-&gt; &#39;rat&#39; &lt;-&gt; &#39;c&#39;</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>websearch_to_tsquery([ &lt;config&gt; regconfig, ] &lt;querytext&gt; text) returns tsquery</span></span></code></pre></div><p><code>websearch_to_tsquery</code> creates a <code>tsquery</code> value from querytext using an alternative syntax in which simple unformatted text is a valid query. Unlike <code>plainto_tsquery</code> and <code>phraseto_tsquery</code>, it also recognizes certain operators. Moreover, this function should never raise syntax errors, which makes it possible to use raw user-supplied input for search. The following syntax is supported:</p><ul><li><p><code>unquoted text</code>: text not inside quote marks will be converted to terms separated by <code>&amp;</code> operators, as if processed by <code>plainto_tsquery</code>.</p></li><li><p><code>&quot;quoted text&quot;</code>: text inside quote marks will be converted to terms separated by <code>&lt;-&gt;</code> operators, as if processed by <code>phraseto_tsquery</code>.</p></li><li><p><code>OR</code>: logical or will be converted to the <code>|</code> operator.</p></li><li><p><code>-</code>: the logical not operator, converted to the <code>!</code> operator.</p></li></ul><p>Examples:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT websearch_to_tsquery(&#39;english&#39;, &#39;The fat rats&#39;);</span></span>
<span class="line"><span> websearch_to_tsquery</span></span>
<span class="line"><span>----------------------</span></span>
<span class="line"><span> &#39;fat&#39; &amp; &#39;rat&#39;</span></span>
<span class="line"><span>(1 row)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>SELECT websearch_to_tsquery(&#39;english&#39;, &#39;&quot;supernovae stars&quot; -crab&#39;);</span></span>
<span class="line"><span>       websearch_to_tsquery</span></span>
<span class="line"><span>----------------------------------</span></span>
<span class="line"><span> &#39;supernova&#39; &lt;-&gt; &#39;star&#39; &amp; !&#39;crab&#39;</span></span>
<span class="line"><span>(1 row)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>SELECT websearch_to_tsquery(&#39;english&#39;, &#39;&quot;sad cat&quot; or &quot;fat rat&quot;&#39;);</span></span>
<span class="line"><span>       websearch_to_tsquery</span></span>
<span class="line"><span>-----------------------------------</span></span>
<span class="line"><span> &#39;sad&#39; &lt;-&gt; &#39;cat&#39; | &#39;fat&#39; &lt;-&gt; &#39;rat&#39;</span></span>
<span class="line"><span>(1 row)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>SELECT websearch_to_tsquery(&#39;english&#39;, &#39;signal -&quot;segmentation fault&quot;&#39;);</span></span>
<span class="line"><span>         websearch_to_tsquery</span></span>
<span class="line"><span>---------------------------------------</span></span>
<span class="line"><span> &#39;signal&#39; &amp; !( &#39;segment&#39; &lt;-&gt; &#39;fault&#39; )</span></span>
<span class="line"><span>(1 row)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>SELECT websearch_to_tsquery(&#39;english&#39;, &#39;&quot;&quot;&quot; )( dummy \\\\ query &lt;-&gt;&#39;);</span></span>
<span class="line"><span> websearch_to_tsquery</span></span>
<span class="line"><span>----------------------</span></span>
<span class="line"><span> &#39;dummi&#39; &amp; &#39;queri&#39;</span></span>
<span class="line"><span>(1 row)</span></span></code></pre></div><h2 id="ranking-search-results" tabindex="-1"><a id="ranking"></a>Ranking Search Results <a class="header-anchor" href="#ranking-search-results" aria-label="Permalink to &quot;&lt;a id=&quot;ranking&quot;&gt;&lt;/a&gt;Ranking Search Results&quot;">​</a></h2><p>Ranking attempts to measure how relevant documents are to a particular query, so that when there are many matches the most relevant ones can be shown first. WarehousePG provides two predefined ranking functions, which take into account lexical, proximity, and structural information; that is, they consider how often the query terms appear in the document, how close together the terms are in the document, and how important is the part of the document where they occur. However, the concept of relevancy is vague and very application-specific. Different applications might require additional information for ranking, e.g., document modification time. The built-in ranking functions are only examples. You can write your own ranking functions and/or combine their results with additional factors to fit your specific needs.</p><p>The two ranking functions currently available are:</p><p><code>ts_rank([ &lt;weights&gt; float4[], ] &lt;vector&gt; tsvector, &lt;query&gt; tsquery [, &lt;normalization&gt; integer ]) returns float4</code> : Ranks vectors based on the frequency of their matching lexemes.</p><p><code>ts_rank_cd([ &lt;weights&gt; float4[], ] &lt;vector&gt; tsvector, &lt;query&gt; tsquery [, &lt;normalization&gt; integer ]) returns float4</code> : This function computes the <em>cover density</em> ranking for the given document vector and query, as described in Clarke, Cormack, and Tudhope&#39;s &quot;Relevance Ranking for One to Three Term Queries&quot; in the journal &quot;Information Processing and Management&quot;, 1999. Cover density is similar to <code>ts_rank</code> ranking except that the proximity of matching lexemes to each other is taken into consideration.</p><p>This function requires lexeme positional information to perform its calculation. Therefore, it ignores any &quot;stripped&quot; lexemes in the <code>tsvector</code>. If there are no unstripped lexemes in the input, the result will be zero. (See <a href="./features.html#manipulating-documents">Manipulating Documents</a> for more information about the <code>strip</code> function and positional information in <code>tsvector</code>s.)</p><p>For both these functions, the optional <code>&lt;weights&gt;</code> argument offers the ability to weigh word instances more or less heavily depending on how they are labeled. The weight arrays specify how heavily to weigh each category of word, in the order:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>{D-weight, C-weight, B-weight, A-weight}</span></span></code></pre></div><p>If no <code>&lt;weights&gt;</code> are provided, then these defaults are used:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>{0.1, 0.2, 0.4, 1.0}</span></span></code></pre></div><p>Typically weights are used to mark words from special areas of the document, like the title or an initial abstract, so they can be treated with more or less importance than words in the document body.</p><p>Since a longer document has a greater chance of containing a query term it is reasonable to take into account document size, e.g., a hundred-word document with five instances of a search word is probably more relevant than a thousand-word document with five instances. Both ranking functions take an integer <code>&lt;normalization&gt;</code> option that specifies whether and how a document&#39;s length should impact its rank. The integer option controls several behaviors, so it is a bit mask: you can specify one or more behaviors using <code>|</code> (for example, <code>2|4</code>).</p><ul><li>0 (the default) ignores the document length</li><li>1 divides the rank by 1 + the logarithm of the document length</li><li>2 divides the rank by the document length</li><li>4 divides the rank by the mean harmonic distance between extents (this is implemented only by <code>ts_rank_cd</code>)</li><li>8 divides the rank by the number of unique words in document</li><li>16 divides the rank by 1 + the logarithm of the number of unique words in document</li><li>32 divides the rank by itself + 1</li></ul><p>If more than one flag bit is specified, the transformations are applied in the order listed.</p><p>It is important to note that the ranking functions do not use any global information, so it is impossible to produce a fair normalization to 1% or 100% as sometimes desired. Normalization option <code>32 (rank/(rank+1))</code> can be applied to scale all ranks into the range zero to one, but of course this is just a cosmetic change; it will not affect the ordering of the search results.</p><p>Here is an example that selects only the ten highest-ranked matches:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT title, ts_rank_cd(textsearch, query) AS rank</span></span>
<span class="line"><span>FROM apod, to_tsquery(&#39;neutrino|(dark &amp; matter)&#39;) query</span></span>
<span class="line"><span>WHERE query @@ textsearch</span></span>
<span class="line"><span>ORDER BY rank DESC</span></span>
<span class="line"><span>LIMIT 10;</span></span>
<span class="line"><span>                     title                     |   rank</span></span>
<span class="line"><span>-----------------------------------------------+----------</span></span>
<span class="line"><span> Neutrinos in the Sun                          |      3.1</span></span>
<span class="line"><span> The Sudbury Neutrino Detector                 |      2.4</span></span>
<span class="line"><span> A MACHO View of Galactic Dark Matter          |  2.01317</span></span>
<span class="line"><span> Hot Gas and Dark Matter                       |  1.91171</span></span>
<span class="line"><span> The Virgo Cluster: Hot Plasma and Dark Matter |  1.90953</span></span>
<span class="line"><span> Rafting for Solar Neutrinos                   |      1.9</span></span>
<span class="line"><span> NGC 4650A: Strange Galaxy and Dark Matter     |  1.85774</span></span>
<span class="line"><span> Hot Gas and Dark Matter                       |   1.6123</span></span>
<span class="line"><span> Ice Fishing for Cosmic Neutrinos              |      1.6</span></span>
<span class="line"><span> Weak Lensing Distorts the Universe            | 0.818218</span></span></code></pre></div><p>This is the same example using normalized ranking:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT title, ts_rank_cd(textsearch, query, 32 /* rank/(rank+1) */ ) AS rank</span></span>
<span class="line"><span>FROM apod, to_tsquery(&#39;neutrino|(dark &amp; matter)&#39;) query</span></span>
<span class="line"><span>WHERE  query @@ textsearch</span></span>
<span class="line"><span>ORDER BY rank DESC</span></span>
<span class="line"><span>LIMIT 10;</span></span>
<span class="line"><span>                     title                     |        rank</span></span>
<span class="line"><span>-----------------------------------------------+-------------------</span></span>
<span class="line"><span> Neutrinos in the Sun                          | 0.756097569485493</span></span>
<span class="line"><span> The Sudbury Neutrino Detector                 | 0.705882361190954</span></span>
<span class="line"><span> A MACHO View of Galactic Dark Matter          | 0.668123210574724</span></span>
<span class="line"><span> Hot Gas and Dark Matter                       |  0.65655958650282</span></span>
<span class="line"><span> The Virgo Cluster: Hot Plasma and Dark Matter | 0.656301290640973</span></span>
<span class="line"><span> Rafting for Solar Neutrinos                   | 0.655172410958162</span></span>
<span class="line"><span> NGC 4650A: Strange Galaxy and Dark Matter     | 0.650072921219637</span></span>
<span class="line"><span> Hot Gas and Dark Matter                       | 0.617195790024749</span></span>
<span class="line"><span> Ice Fishing for Cosmic Neutrinos              | 0.615384618911517</span></span>
<span class="line"><span> Weak Lensing Distorts the Universe            | 0.450010798361481</span></span></code></pre></div><p>Ranking can be expensive since it requires consulting the tsvector of each matching document, which can be I/O bound and therefore slow. Unfortunately, it is almost impossible to avoid since practical queries often result in large numbers of matches.</p><h2 id="highlighting-results" tabindex="-1"><a id="highlighting"></a>Highlighting Results <a class="header-anchor" href="#highlighting-results" aria-label="Permalink to &quot;&lt;a id=&quot;highlighting&quot;&gt;&lt;/a&gt;Highlighting Results&quot;">​</a></h2><p>To present search results it is ideal to show a part of each document and how it is related to the query. Usually, search engines show fragments of the document with marked search terms. WarehousePG provides a function <code>ts_headline</code> that implements this functionality.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ts_headline([&lt;config&gt; regconfig, ] &lt;document&gt; text, &lt;query&gt; tsquery [, &lt;options&gt; text ]) returns text</span></span></code></pre></div><p><code>ts_headline</code> accepts a document along with a query, and returns an excerpt from the document in which terms from the query are highlighted. The configuration to be used to parse the document can be specified by <code>*config*</code>; if <code>*config*</code> is omitted, the <code>default_text_search_config</code> configuration is used.</p><p>If an <code>*options*</code> string is specified it must consist of a comma-separated list of one or more <code>*option=value*</code> pairs. The available options are:</p><ul><li><code>MaxWords</code>, <code>MinWords</code> (integers): these numbers determine the longest and shortest headlines to output. The default values are 35 and 15.</li><li><code>ShortWord</code> (integer): words of this length or less will be dropped at the start and end of a headline, unless they are query terms. The default value of three eliminates common English articles.</li><li><code>HighlightAll</code> (boolean): the whole document will be used as the headline, ignoring the preceding three parameters. The default is <code>false</code>.</li><li><code>MaxFragments</code> (integer): maximum number of text fragments to display. The default value of zero selects a non-fragment-based headline generation method. A value greater than zero selects fragment-based headline generation (see below).</li><li><code>StartSel</code>, <code>StopSel</code> (strings): the strings with which to delimit query words appearing in the document, to distinguish them from other excerpted words. The default values are &quot;<code>&lt;b&gt;</code>&quot; and &quot;<code>&lt;/b&gt;</code>&quot;, which can be suitable for HTML output.</li><li><code>FragmentDelimiter</code> (string): When more than one fragment is displayed, the fragments will be separated by this string. The default is &quot;<code>...</code>&quot;.</li></ul><p>These option names are recognized case-insensitively. You must double-quote string values if they contain spaces or commas.</p><p>In non-fragment-based headline generation, <code>ts_headline</code> locates matches for the given <code>&lt;query&gt;</code> and chooses a single one to display, preferring matches that have more query words within the allowed headline length. In fragment-based headline generation, <code>ts_headline</code> locates the query matches and splits each match into “fragments” of no more than <code>MaxWords</code> words each, preferring fragments with more query words, and when possible “stretching” fragments to include surrounding words. The fragment-based mode is thus more useful when the query matches span large sections of the document, or when it&#39;s desirable to display multiple matches. In either mode, if no query matches can be identified, then a single fragment of the first <code>MinWords</code> words in the document will be displayed.</p><p>For example:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>SELECT ts_headline(&#39;english&#39;,</span></span>
<span class="line"><span>  &#39;The most common type of search</span></span>
<span class="line"><span>is to find all documents containing given query terms</span></span>
<span class="line"><span>and return them in order of their similarity to the</span></span>
<span class="line"><span>query.&#39;,</span></span>
<span class="line"><span>  to_tsquery(&#39;english&#39;, &#39;query &amp; similarity&#39;));</span></span>
<span class="line"><span>                        ts_headline</span></span>
<span class="line"><span>------------------------------------------------------------</span></span>
<span class="line"><span> containing given &lt;b&gt;query&lt;/b&gt; terms                       +</span></span>
<span class="line"><span> and return them in order of their &lt;b&gt;similarity&lt;/b&gt; to the+</span></span>
<span class="line"><span> &lt;b&gt;query&lt;/b&gt;.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>SELECT ts_headline(&#39;english&#39;,</span></span>
<span class="line"><span>  &#39;Search terms may occur</span></span>
<span class="line"><span>many times in a document,</span></span>
<span class="line"><span>requiring ranking of the search matches to decide which</span></span>
<span class="line"><span>occurrences to display in the result.&#39;,</span></span>
<span class="line"><span>  to_tsquery(&#39;english&#39;, &#39;search &amp; term&#39;),</span></span>
<span class="line"><span>  &#39;MaxFragments=10, MaxWords=7, MinWords=3, StartSel=&lt;&lt;, StopSel=&gt;&gt;&#39;);</span></span>
<span class="line"><span>                        ts_headline</span></span>
<span class="line"><span>------------------------------------------------------------</span></span>
<span class="line"><span> &lt;&lt;Search&gt;&gt; &lt;&lt;terms&gt;&gt; may occur                            +</span></span>
<span class="line"><span> many times ... ranking of the &lt;&lt;search&gt;&gt; matches to decide</span></span></code></pre></div><p><code>ts_headline</code> uses the original document, not a <code>tsvector</code> summary, so it can be slow and should be used with care.</p><p><strong>Parent topic:</strong> <a href="./../textsearch/full-text-search.html">Using Full Text Search</a></p>`,80)]))}const g=s(o,[["render",i]]);export{u as __pageData,g as default};
